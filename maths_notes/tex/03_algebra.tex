\section{Algebra} \label{sec_algebra}

\subsection{Fundamental Theorem of Algebra}

\subsection{Difference of two Squares}

A difference of squares can be factorised
\begin{equation*}
 a^2 - b^2 \equiv (a + b)(a - b)
\end{equation*}
This can be verified with fairly simple
algebra.

\subsubsection{Difference of Higher Powers}

Similar results can be found in higher powers of \(a\) and \(b\):
\begin{align*}
 a^3 - b^3 &\equiv (a - b)(a^2 + ab + b^2) \\
 a^4 - b^4 &\equiv (a - b)(a^3 + a^2b + b^2a + b^3) \\
 \dots
\end{align*}
In general, you take out a factor of \((a - b)\) and then start with the
term \(a^{n - 1}\), and for each subsequent term decrease the power in \(a\)
and increase the power in \(b\):
\begin{equation*}
 a^n - b^n \equiv (a - b)(a^{n - 1} + a^{n - 2}b + a^{n - 3}b^2 + \dotsb +
                         ab^{n - 2} + b^{n - 1})
\end{equation*}
This can in fact be derived from the partial sum of a geometric progression
(\ref{sec_seq_GP}). The sum \(a^n + a^{n - 1}b + \dotsb + ab^{n - 1} + b^n\) is
a geometric progression of \(n + 1\) terms with first term \(a^n\) and
common ratio \(b/a\). Therefore,
\begin{alignat*}2
 &&a^n + a^{n - 1}b + \dotsb + ab^{n - 1} + b^n &=
         a^n \cdot \frac{(b / a)^{n + 1} - 1}{(b / a) - 1} \\
 &&    &= \frac{a^{n + 1} - b^{n + 1}}{a - b} \\
 &\implies{}& (a - b)(a^n + a^{n - 1}b + \dotsb + ab^{n - 1} + b^n) &=
         a^{n + 1} - b^{n + 1}
\end{alignat*}

Note that for odd \(n\), \(a^n + b^n\) can also be factorised as
\(a^n + b^n \equiv a^n - (-b)^n\), so you get the same expansion but with
alternating positive and negative terms.

% FIXME: arguments from totient function and with different powers? number
% of common factors?

\subsubsection{Difference of Composite Powers}

In fact, in the last section, \(a^4 - b^4\) may be more fully factorised by
using:
\begin{equation*}
 a^4 - b^4 \equiv (a^2)^2 - (b^2)^2 \equiv (a^2 - b^2)(a^2 + b^2) \equiv
     (a - b)(a + b)(a^2 + b^2)
\end{equation*}
This happens as \(4\) is composite. Any composite \(n\) will in fact result
in the cototient (\ref{sec_totient}) of \(n = n - \phi(n)\) factors (at
least as far as I can see, but probably can't prove). This
also holds for prime \(n\), but for prime \(n\),
\(\phi(n) = n - 2 \implies n - \phi(n) = 2\), as the only divisors of
a prime \(n\) are \(1\) and \(n\).

We may derive this from the fact that any
\(n = pq \colon p, q \in \setstyle P\),
\(a^n - b^n\) will factorise as
\begin{equation*}
 a^{pq} - b^{pq} \equiv
     (a^p - b^p)(a^{p(q - 1)} + a^{p(q - 2)}b^{p(1)} + \dotsb +
                 a^{p(1)}b^{p(q - 2)} + b^{p(q - 1)}
\end{equation*}

\subsection{Quadratic formula} \label{sec_quad_formula}

There is a formula to give the roots of a general quadratic.
\begin{theorem}[Quadratic formula]
 \label{thm_quad_form}
 \begin{align*}
  ax^2 + bx + c &= 0\ \text{where}\ a \neq 0 \\
  \iff x &= \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
 \end{align*}
\end{theorem}
\begin{proof}
 We can complete the square to solve a general quadratic equation.
 \begin{alignat*}2
  &&ax^2 + bx + c &= 0 \\
  &\iff{}& x^2 + \frac ba x + \frac ca &= 0 \\
  &\iff{}& \parens[\Big]{x + \frac b{2a}}^2
      &= \parens[\Big]{\frac b{2a}}^2 - \frac ca
      = \frac{b^2 - 4ac}{4a^2} \\
  &\iff{}& x + \frac b{2a} &= \pm \sqrt{\frac{b^2 - 4ac}{4a^2}}
      = \pm \frac{\sqrt{b^2 - 4ac}}{2a} \\
  &\iff{}& x &= \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \qedhere
 \end{alignat*}
\end{proof}
Notational note: the notation \(\bullet_0 = \bullet_1 \pm \bullet_2\) is really
a shorthand for
\(\bullet_0 \in \set{\bullet_1 + \bullet_2, \bullet_1 - \bullet_2}\), where
\(\bullet_i\) are placeholders for expressions. This means it becomes fairly
important to keep a handle on the direction that your implications are going.
Generally, the \(\pm\) ``operator'' behaves similarly to addition -
multiplication and division distribute over it, for example.

The symbol can occur multiple times in an expression, which usually means you
should just convert them all to \(+\) once, and to \(-\) once - that is, it does
not exhibit branching behaviour. If that's what you want to show, probably
better off just explicitly writing that out. Occasionally
\(\bullet_0 \mp \bullet_1\) is used as a shorthand for
\(\bullet_0 \pm -\bullet_1\)
(eg \(x = 1 \pm 2 \mp 3 \iff x = 1 \pm -1\)).

Confusingly, it is also used inside a set to indicate an element in addition to
its negation, sometimes:
\(\set{\pm \bullet, \cdots} \defeq \set{\bullet, -\bullet, \cdots}\).

Back to business. A corollary of Theorem \ref{thm_quad_form} is that the number
of real roots of a quadratic is
determined by the discriminant \({\Delta = b^2 - 4ac}\).
\begin{corollary}[Quadratic real roots]
 Where \(P(x) \defeq ax^2 + bx + c\) and \(\Delta \defeq b^2 - 4ac\),
 \begin{align*}
  \Delta &= 0 \iff \text{``\(P(x) = 0\) has one repeated real root''} \\
  \Delta &< 0 \iff \text{``\(P(x) = 0\) has no real roots''} \\
  \Delta &> 0 \iff \text{``\(P(x) = 0\) has two real roots''}
 \end{align*}
\end{corollary}

\subsection{Partial Fraction Decomposition}

When you have a large rational function, it can often be useful to decompose it
into a sum of smaller rational functions.

\subsection[Cauchy-Shwarz inequality for
            \texorpdfstring{\(\Reals^n\)}{sequences of real numbers}]
           {Cauchy-Shwarz inequality for \boldmath\(\Reals^n\)}

\begin{theorem}[Cauchy-Shwarz inequality]
 For two sequences of length \(n\), \(u_i, v_i \in \Reals\), indexed by
 \(i \in I\) where \(I = \set{1, 2, \dotsc, n}\),
 \begin{equation*}
  \parens[\Big]{\sum u_i v_i}^2 \le
      \parens[\Big]{\sum u_i^2} \parens[\Big]{\sum v_i^2}
 \end{equation*}
 with equality iff
 \(\Exists k \in \Reals \colon \Forall i \in I \colon u_i = k v_i\)
 (ie one sequence is a multiple of the other).
\end{theorem}
\begin{proof}
    We consider the polynomial
    \begin{equation*}
    P(x) = \sum (u_i x + v_i)^2 = 0
    \end{equation*}
    If there is any \(u_i\) term which is nonzero, this is a quadratic in \(x\)
    (if this condition is not met, the inequality becomes obviously true with
    equality).
    \begin{equation*}
     \parens[\Big]{\sum u_i^2} x^2 +
     \parens[\Big]{\sum 2 u_i v_i} x + \sum v_i^2 = 0
    \end{equation*}
    As it is a sum of squares of real terms, it must be nonnegative. In fact, it
    can only be zero if each contributing term has precisely the same zero,
    which happens only if all \(-v_i/u_i\) are equal, which leads to the
    condition for equality.

    As it has no zeroes or one zero (in the case of the condition for equality)
    \begin{alignat*}2
     &&\Delta = b^2 - 4ac &\le 0 \\
     &\iff{}&
     \parens[\Big]{\sum 2 u_i v_i}^2 -
                4\parens[\Big]{\sum u_i^2}\parens[\Big]{\sum v_i^2} &\le 0 \\
     &\iff{}& \parens[\Big]{\sum u_i v_i}^2
         &\le \parens[\Big]{\sum u_i^2} \parens[\Big]{\sum v_i^2} \qedhere
    \end{alignat*}
\end{proof}

\subsection{AM-GM inequality}

\begin{theorem}[AM-GM inequality]
 For a sequence \(u_i \in \Reals\), for \(1 \le i \le n\) (ie, the
 sequence is of length \(n\)) and \(u_i \ge 0\),
 \begin{equation*}
  \sqrt[n]{u_1 u_2 \dotsm u_n} \le \frac{u_1 + u_2 + \dotsb + u_n}n
 \end{equation*}
 with equality iff all \(u_i\) are equal.

 Equivalently, using sigma and pi notation:
 \begin{equation*}
  \parens[\Big]{\prod u_i}^\frac 1n \le \frac 1n\sum u_i
 \end{equation*}
\end{theorem}
\begin{proof}
 We can use a kind of wonky induction. First, we verify the base
 case, \(n = 2\):
 \begin{alignat*}2
  &&\sqrt{ab} &\le \frac{a + b}2 \\
  &\iff{}& 4ab &\le a^2 + 2ab + b^2 \\
  &\iff{}& 0 &\le a^2 - 2ab + b^2 \\
  &\iff{}& 0 &\le (a - b)^2\quad \text{with equality iff \(a = b\)}
 \end{alignat*}
 Then, supposing AM-GM holds for \(n\) and 2, we show that it holds for
 \(2n\). Taking
 \begin{alignat*}3
  &&a &= \sqrt[n]{u_1 u_2 \dotsm u_n} \\
  &&b &= \sqrt[n]{u_{n+1} u_{n+2} \dotsm u_{2n}} \\
  &\implies{}& a &\le \frac{u_1 + u_2 + \dotsb + u_n}n
          &&\quad \text{with equality iff \(u_1 = \dotsb = u_n\)}\\
  &&b &\le \frac{u_{n + 1} + u_{n + 2} + \dotsb + u_{2n}}n
          &&\quad \text{with equality iff \(u_{n+1} = \dotsb = u_{2n}\)}\\
  &&\text{and}\ \sqrt{ab} &\le \frac{a + b}2
      &&\quad \text{with equality iff \(a = b\)}\\
  &\implies{}& \sqrt[2n]{u_1 u_2 \dotsm u_{2n}} &\le
          \frac{u_1 + u_2 + \dotsb u_{2n}}{2n}
          &&\quad \text{with equality iff \(u_1 = \dotsb = u_{2n}\)}
 \end{alignat*}
 Now, supposing AM-GM holds for \(n\), we show that it holds for \(n - 1\).
 Taking
 \begin{alignat*}2
  &&u_n &= \sqrt[n - 1]{u_1 u_2 \dotsm u_{n - 1}} \\
  &\implies{}& (u_1 u_2 \dotsm u_n)^{1 / n}
          &= (u_1 u_2 \dotsm u_{n - 1})^{1 / (n - 1)} \\
  &\implies{}& (u_1 u_2 \dotsm u_{n - 1})^{1 / (n - 1)} &\le
          \frac 1n (u_1 + u_2 + \dotsb u_{n - 1}) +
          \frac 1n (u_1 u_2 \dotsm u_{n - 1})^{1 / (n - 1)} \\
  &\implies{}& \parens[\Big]{1 - \frac 1n}
      (u_1 u_2 \dotsm u_{n - 1})^{1 / (n - 1)} &\le
      \frac 1n (u_1 + u_2 + \dotsb + u_{n - 1}) \\
  &\implies{}& \frac {n - 1}n
          (u_1 u_2 \dotsb u_{n - 1})^{1 / (n - 1)} &\le
          \frac 1n (u_1 + u_2 + \dotsb + u_{n - 1}) \\
  &\implies{}& (u_1 u_2 \dotsm u_{n - 1})^{1 / (n - 1)} &\le
          \frac 1{n - 1} (u_1 + u_2 + \dotsb + u_{n - 1})
 \end{alignat*}
 As equality for \(n\) was iff all \(u_i\) were the same, this is still true.
 Now, for any \(n \in \Integers^+\), we can induct up to a power of 2 above
 \(n\), and then descend from there, so we know \(P(n)\) is true for all
 \(n \in \Integers^+\).
\end{proof}

\subsubsection{Generalized Power Means}

\subsection{de Moivre's Theorem}

\begin{theorem}[de Moivre's Theorem]
 \begin{equation*}
  (\cos \theta + i \sin \theta)^n \equiv \cos n\theta + i \sin n\theta
 \end{equation*}
\end{theorem}
\begin{proof}
 This can be proven for the integers by induction. Let \(P(n)\) denote de
 Moivre's theorem for \(\Forall n \in \Integers_0^+\).
 \begin{enumerate}[I.]
  \item \label{basec_thm_demoivre} Consider \(P(0)\):
        \begin{equation*}
         (\cos \theta + i \sin \theta)^0 \equiv 1 \equiv \cos 0 + i \sin 0
        \end{equation*}
  \item \label{induct_thm_demoivre} Now suppose \(P(n)\) is true and consider
        \(P(n + 1)\):
        \begin{align*}
         (\cos \theta + i \sin \theta)^{n + 1} &\equiv
          (\cos \theta + i \sin \theta)(\cos \theta + i \sin \theta)^n \\
          &\equiv (\cos \theta + i \sin \theta)
                  (\cos n\theta + i \sin n\theta) \impliedby P(n) \\
          &\equiv \cos \theta \cos n\theta - \sin \theta \sin n\theta
                + i(\cos \theta \sin n\theta + \sin \theta \cos n\theta) \\
          &\equiv \cos(\theta + n\theta) + i \sin(\theta + n\theta) \\
          &\equiv \cos (n + 1)\theta + i \sin (n + 1)\theta
        \end{align*}
        So \(P(n + 1)\) is true if \(P(n)\) is true.
  \item By the principle of mathematical induction, \ref{basec_thm_demoivre} and
        \ref{induct_thm_demoivre} imply that \(P(n)\) is true for
        \(\Forall n \in \Integers_0^+\).
 \end{enumerate}
\end{proof}

\subsection{Euler's Formula}

\begin{theorem}[Euler's Formula]
 \begin{equation*}
  e^{i\theta} \equiv \cos \theta + i \sin \theta
 \end{equation*}
\end{theorem}
\begin{proof}
 The standard, though fairly non-intuitive proof of this fact comes from a
 consideration of power series. Using the Maclaurin series of \(e^x\), we get
 \begin{alignat*}2
  e^{i\theta} &= \sum_{r = 0}^\infty \frac{(i\theta)^r}{r!}
            &{}={}& \frac{(i\theta)^0}{0!} + \frac{(i\theta)^1}{1!}
               + \frac{(i\theta)^2}{2!} + \frac{(i\theta)^3}{3!} + \dotsb \\
              &= \sum_{r = 0}^\infty \frac{(i\theta)^{2r}}{(2r)!}
               + \sum_{r = 0}^\infty \frac{(i\theta)^{2r + 1}}{(2r + 1)!}
            &{}={}& \parens[\Bigg]{
                  \frac{(i\theta)^0}{0!} + \frac{(i\theta)^2}{2!} + \dotsb}
               + \parens[\Bigg]{
                  \frac{(i\theta)^1}{1!} + \frac{(i\theta)^3}{3!} + \dotsb} \\
              &= \sum_{r = 0}^\infty \frac{(-1)^r\theta^{2r}}{(2r)!}
               + i\sum_{r = 0}^\infty \frac{(-1)^r\theta^{2r + 1}}{(2r + 1)!}
            &{}={}& \parens[\Bigg]{
                  1 - \frac{\theta^2}{2!}
                + \frac{\theta^4}{4!} - \dotsb}
               + i\parens[\Bigg]{
                  \theta - \frac{\theta^3}{3!}
                + \frac{\theta^5}{5!} - \dotsb} \\
              &= \cos \theta + i \sin \theta
 \end{alignat*}
 Here the reordering of the series \emph{is} justified as each series is
 absolutely convergent. % TODO
\end{proof}
\begin{proof}
 A way I find more intuitive to understand is by considering the function
 \(e^x\) as the unique solution to the differential equation
 \begin{equation*}
  \dv<y>{x} = y
 \end{equation*}
 with \(y(0) = 1\). Using the chain rule, it can be seen that
 \(f(\theta) = e^{i\theta}\) must satisfy
 \begin{equation*}
  f'(\theta) = if(\theta)
 \end{equation*}
 % TODO expansion and diagrams
 This means that the rate at which \(f(\theta)\) is changing is \(f(\theta)\)
 multiplied by \(i\). In the complex plane, multiplication by \(i\) corresponds
 to rotation by \(\SI{90}{\degree}\). Thinking of the derivative of \(f\) as the
 velocity of the point described in the plane by \(f(t)\), we see that its
 tangential velocity is always perpendicular to its position vector with respect
 to the origin. Furthermore, by differentiating again we can show that
 \(f''(\theta) = -f(\theta)\), ie its acceleration is always centripetal. This,
 to me, explains why \(f(\theta)\) should be a circle. I first encountered this
 explanation in \cite{3B1BEToTheIPi}.

 A more rigorous (wordy) procedure from here might be as follows. If we suppose
 that \(f\) is complex-valued - ie for any \(\theta\),
 \(f(\theta) \in \Complex\), then we can decompose \(f(\theta)\) into its real
 and imaginary parts. This assumption will be justified by the fact that we
 later get a solution, but it might also be justified by considering again the
 Maclaurin series of \(e^{i\theta}\), noting that it is a sum of complex and
 real numbers, and so must be complex valued. A less rigorous approach is to
 note that \(f(0) = 1 \in \Reals\), and \(f'(0) = i \in \Complex\). Then the
 first ``step'' the function takes is a complex-valued one, but then so must be
 any subsequent step, as the commmplex numbers are closed under multiplication.
  Anyway,
 \begin{equation*}
  f(\theta) = g(\theta) + i h(\theta)
   \quad \text{where} \quad
    g(\theta) = \Re(f(\theta)),\quad h(\theta) = \Im(f(\theta))
 \end{equation*}
 But
 \begin{alignat*}2
  && f'(\theta) &= if(\theta) = -h(\theta) + ig(\theta)
                 = g'(\theta) + ih'(\theta) \\
  &\implies{}&
   % TODO: centre asterisks
      g'(\theta) &= -h(\theta) \tag{\(\ast\)} \label{eqn_thm_euler_gh} \\
  &&  h'(\theta) &= -g(\theta) \tag{\(\ast\ast\)} \label{eqn_thm_euler_hg}
 \end{alignat*}
    By differentiating \ref{eqn_thm_euler_gh}, and then substituting in
    \ref{eqn_thm_euler_hg}, we get
    \begin{equation*}
     g''(\theta) = -h'(\theta) = -g(\theta)
    \end{equation*}
    Symmetrically,
    \begin{equation*}
     h''(\theta) = -g'(\theta) = -h(\theta)
    \end{equation*}
    Noting that \(f(0) = 1\), we have the initial conditions
    \begin{equation*}
     g(0) = 1, \quad h(0) = 0
    \end{equation*}
    This is sufficient to determine that \(g(\theta) = \cos \theta\), and
    subsequently \(h(\theta) = -\dv{\theta}(\cos \theta) = \sin \theta\).
\end{proof}
\begin{proof}[Proof from the exponential limit definition]
 We can use the definition
 \begin{equation*}
  e^x = \lim_{n \to \infty} \parens[\Big]{1 + \frac xn}^n
 \end{equation*}
 to form a geometric construction in the complex plane of \(e^{i \theta}\).

 Using some properties of complex numbers written in polar form under
 exponentiation, we can deduce the modulus and argument of the result in order
 to deduce its value.

 Consider first the argument.
 \begin{align*}
  \arg e^{i \theta} &= \arg \lim_{n \to \infty} \parens[\Big]
                             {1 + \frac{i \theta}n}^n \\
                    &= \lim_{n \to \infty} \arg \parens[\Big]
                             {1 + \frac{i \theta}n}^n \\
                    &= \lim_{n \to \infty} n\parens[\Big]{\arg \parens[\Big]
                             {1 + \frac{i \theta}n}} \\
                    &= \lim_{n \to \infty} n \arctan \frac \theta n \\
                    &= \lim_{n \to \infty} n \cdot \frac \theta n \\
                    &= \lim_{n \to \infty} \theta \\
                    &= \theta
 \end{align*}
 Consider now the modulus.
 \begin{align*}
  \abs{e^{i \theta}}^2 &= \abs[\Big]{\lim_{n \to \infty} \parens[\Big]
                            {1 + \frac{i \theta}n}^n}^2 \\
                       &= \lim_{n \to \infty} \abs[\Big]{\parens[\Big]
                            {1 + \frac{i \theta}n}^n}^2 \\
                       &= \lim_{n \to \infty} \abs[\Big]{\parens[\Big]
                            {1 + \frac{i \theta}n}}^{2n} \\
                       &= \lim_{n \to \infty} \parens[\Big]
                          {1 + \parens[\Big]{\frac \theta n}^2}^n \\
                       &= \lim_{n \to \infty} \parens[\Big]
                          {\frac{n^2 + \theta^2}{n^2}}^n \\
                       &= \lim_{n \to \infty} \parens[\Big]
                          {\frac
                           {\sum_{r = 0}^n \binom nr n^{2(n - r)} \theta^{2r}}
                           {n^{2n}}} \\
                       &= \lim_{n \to \infty} \parens[\Big]
                          {\frac
                           {n^{2n} +
                            \sum_{r = 1}^n \binom nr n^{2(n - r)} \theta^{2r}}
                           {n^{2n}}} \\
                       &= \lim_{n \to \infty} \parens[\Big]
                          {1 + \frac
                           {\sum_{r = 1}^n \binom nr n^{2(n - r)} \theta^{2r}}
                           {n^{2n}}} \\
                       &= 1 \quad \text{(see Lemma \ref{lem_nn_exp_limit})}
 \end{align*}
 So indeed, \(e^{i \theta} = \cos \theta + i \sin \theta\).
 % TODO: diagram
\end{proof}
\begin{proof}[Proof by polar coordinates]
 I found this proof first in \cite{WikiEulersFormula}.

 If we suppose that \(e^{i \theta} \in \Complex\), then it can be expressed in
 polar form:
 \begin{equation*}
  e^{i \theta} = \rho (\cos \phi + i \sin \phi)
 \end{equation*}
 Where \(\rho\) and \(\phi\) are (currently) undetermined functions of
 \(\theta\). We differentiate on each side with respect to \(\theta\):
 \begin{equation*}
  i e^{i \theta} = \dv<\rho>{\theta} (\cos \phi + i \sin \phi)
                 + \dv<\phi>{\theta} \rho (-\sin \phi + i \cos \phi)
 \end{equation*}
 We can now use make use of the substitution
 \(e^{i \theta} = \rho (\cos \phi + i \sin \phi)\)
 \begin{alignat*}4
  && i \rho (\cos \phi + i \sin \phi) &=
                   \dv<\rho>{\theta} (\cos \phi + i \sin \phi)
                 + \dv<\phi>{\theta} \rho (-\sin \phi + i \cos \phi) \\
  &\implies{}& i \rho \cos \phi - \rho \sin \phi &=
                   \dv<\rho>{\theta} (\cos \phi + i \sin \phi)
                 + \dv<\phi>{\theta} \rho (-\sin \phi + i \cos \phi)
 \end{alignat*}
 Equating real and imaginary parts, we get
 \begin{alignat*}2
  && -\rho \sin \phi &= \dv<\rho>{\theta} \cos \phi
                      - \rho \dv<\phi>{\theta} \sin \phi \\
  && \rho \cos \phi &= \dv<\rho>{\theta} \sin \phi
                     + \rho \dv<\phi>{\theta} \cos \phi \\
  &\implies{}& \rho \sin^2 \phi \parens[\Big]{\dv<\phi>{\theta} - 1} &=
               \rho \cos^2 \phi \parens[\Big]{1 - \dv<\phi>{\theta}} \\
  && \cos \phi \parens[\Big]{\dv<\rho>{\theta} \cos \phi + \rho \sin \phi} &=
     \sin \phi \parens[\Big]{\rho \cos \phi - \dv<\rho>{\theta} \sin \phi} \\
  &\implies{}& \rho \parens[\Big]{\dv<\phi>{\theta} - 1} &= 0
   % TODO: centre asterisks
   \tag{\(\ast\)} \label{eq_euler_polar_1} \\
  && \dv<\rho>{\theta} &= 0
   \tag{\(\ast\ast\)} \label{eq_euler_polar_2}
 \end{alignat*}
 From \ref{eq_euler_polar_1}, we determine that \(\dv<\phi>{\theta} = 1\), as
 there exist values of \(\theta\) for which \(\rho \ne 0\)
 (eg \(\theta = 0 \implies \rho = \abs{e^{i \cdot 0}} = 1\)). So \(\phi\) is a
 function of the form \(\theta + C\).
 \(\abs{e^{i \cdot 0}} = 1\). Noting that \(\arg e^{i \cdot 0} = 0\), it is
 clear that \(\phi = \theta\).

 Even more immediately, from \ref{eq_euler_polar_2} we see that \(\rho\) is a
 constant function of \(\theta\). As we showed earlier, for \(\theta = 0\)
 \(\rho = 1\) so \(\rho = 1\) for all \(\theta\), and therefore
 \begin{equation*}
  e^{i \theta} = 1 (\cos \theta + i \sin \theta) \qedhere
 \end{equation*}
\end{proof}

\begin{lemma}[Limit of \(n^{an} n^{-bn}\) as \(n \to \infty\), where \(a < b\)]
 \label{lem_nn_exp_limit}
 This corresponds to
 \begin{align*}
  \lim_{n \to \infty} n^{n(a - b)}
   &= \lim_{n \to \infty} e^{(a - b)n \ln n} \\
   &= 0
 \end{align*}
 because \(a < b \implies a - b < 0 \implies e^{a - b} < 1\), and
 \(n \ln n \to \infty\) as \(n \to \infty\).
\end{lemma}

\begin{corollary}[Euler's Identity]
 By simply taking \(\theta = \pi\), we get the famous ``Euler's identity'':
 \begin{equation*}
  e^{i \pi} = \cos \pi + i \sin \pi = -1
 \end{equation*}
\end{corollary}

\subsection[The \texorpdfstring{\(\Gamma\)}{Gamma} function]
           {The \boldmath\(\Gamma\) function}

The ``gamma'' or \(\Gamma\) function is defined for
\(z \in \Complex, \Re(z) > 0\) as
\begin{equation*}
 \Gamma(z) = \integ[0]<\infty>{x^{z - 1}e^{-x}}{x}
\end{equation*}
By integrating by parts, we show the following:
\begin{align*}
 \Gamma(z + 1) &= \integ[0]<\infty>{x^z e^{-x}}{x} \\
           &= -x^z e^{-x}\eval_0^\infty
              + \integ[0]<\infty>{zx^{z - 1}e^{-x}}{x} \\
           &= z\Gamma(z)
\end{align*}
Noting also that
\begin{align*}
 \Gamma(1) &= \integ[0]<\infty>{e^{-x}}{x} \\
       &= -e^{-x}\eval_0^\infty \\
       &= 1
\end{align*}
it can be seen from this recurrence that where \(n \in \Integers^+\),
\begin{equation*}
 \Gamma(n) = (n - 1)!
\end{equation*}
In fact, the gamma function is used as an extension of the idea of
factorials to the real and complex numbers other than the negative integers.

An interesting value that the gamma function takes is for \(z = \frac 12\).
\begin{equation*}
 \Gamma(\tfrac 12) = \integ[0]<\infty>{x^{-\frac 12} e^{-x}}{x}
\end{equation*}
Letting \(x = u^2\)
\begin{alignat*}2
 &\implies{}& \dv<x>{u} &= 2u \\
 &\implies{}& \Gamma(\tfrac 12) &=
     \integ[0]<\infty>{\frac{2u}u e^{-u^2}}{u} \\
 &&  &= 2\integ[0]<\infty>{e^{-u^2}}{u} \\
 &&  &= \sqrt \pi
\end{alignat*}
from Theorem \ref{thm_gauss_integral}. From this we can derive the value of
any \(\Gamma(n + \frac 12)\) where \(n \in \Integers\) from the recurrence
relation on \(\Gamma\), and in some very informal sense, we can find that
the ``factorials'' of the half-integers are rational multiples of the square
root of pi. The first few are shown in Table \ref{tab_gamma_halves}.
\begin{longtable}{*2M}
 \toprule
 \text{\boldmath\(z\)}
     & \text{\bfseries\boldmath\(\Gamma(z)\), AKA ``\boldmath\((z - 1)!\)''} \\
 \midrule
 \endhead
 \bottomrule
 \endfoot
 \endlastfoot
 \rule{0pt}{4ex}
 \frac 12 & \sqrt{\pi} \\[3ex]
 \frac 32 & \frac{\sqrt{\pi}}2 \\[3ex]
 \frac 52 & \frac{3 \sqrt{\pi}}4 \\[3ex]
 \frac 72 & \frac{15 \sqrt{\pi}}8 \\[3ex]
 \frac 92 & \frac{105 \sqrt{\pi}}{16} \\[3ex]
 \multicolumn 2c{\(\cdots\)} \\
 \bottomrule
 \caption{Half-integer values of the gamma function
 \label{tab_gamma_halves}}
\end{longtable}
