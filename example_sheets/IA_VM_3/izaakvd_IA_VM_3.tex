% Compiling with
% latexmk -halt-on-error -shell-escape -synctex=1 -pdf
% (Recommend using a latexmkrc file so as just to run latexmk -pvc, for example)
% Probably you can achieve the same with an inordinate number of invocations of
% pdflatex -halt-on-error -shell-escape -synctex=1

% fleqn aligns equations to the left, a4 paper size, 11pt font, article class
\documentclass[fleqn,a4paper,11pt]{article}
\title{IA Vectors and Matrices Example Sheet 3 (revised)}
\author{Izaak van Dongen (\texttt{imv26})}

\usepackage{mymaths}
\usepackage{mystyle}

\begin{document}
 \maketitle\thispagestyle{empty} % no page number under title

 \begin{enumerate}[label=\textbf{\arabic*.}]
  \item
   As \(\mat A\) and \(\mat B\) are hermitian, generally we have
   \( A_{ij} = \conj{ A_{ji}}\), and
   \( B_{ij} = \conj{ B_{ji}}\).
   \begin{enumerate}[label=(\roman*)]
    \item
     \begin{itemize}
      \item
       \(\tr \mat A =  A_{ii} = \conj{ A_{ii}} = \conj{\tr \mat A}\), as
       complex conjugation distributes over addition, and therefore
       \(\Im(\tr \mat A) = \tfrac 12 (\tr \mat A - \conj{\tr \mat A}) = 0\), ie
       \(\tr \mat A\) is real.
      \item
       Similarly,
       \begin{align*}
        \det \mat A
         &= \sum_\sigma \epsilon(\sigma) \prod_i  A_{i\,\sigma(i)} \\
         &= \sum_\sigma \epsilon(\sigma) \prod_i \conj{ A_{\sigma(i)\,i}} \\
         &= \conj{\sum_\sigma \epsilon(\sigma) \prod_i  A_{\sigma(i)\,i}} \\
         &= \conj{\sum_{\sigma^{-1}} \epsilon(\sigma^{-1})
                 \prod_i  A_{i\,\sigma^{-1}(i)}} \\
         &= \conj{\det \mat A},\quad
         \text{as the sum remains over all permutations \footnotemark}
       \end{align*}
       \footnotetext{
        ie if we let \(\tau = \sigma^{-1}\) then the sum is over all
        permutations \(\tau\), as inversion is a bijection from
        \(S_n \to S_n\).
       }
       so \(\det \mat A\) must be real.
     \end{itemize}
    \item \(
     \begin{aligned}[t]
     (\mat A\mat B + \mat B\mat A)_{ij}
       &=  A_{ik} B_{kj} +  B_{ik} A_{kj} \\
       &= \conj{ A_{ki}} \cdot \conj{ B_{jk}}
          + \conj{ B_{ki}} \cdot \conj{ A_{jk}}
       = \conj{ A_{ki} B_{jk} +  B_{ki} A_{jk}} \\
       &= \conj{ A_{jk} B_{ki} +  B_{jk} A_{ki}} \\
       &= \conj{(\mat A\mat B + \mat B\mat A)_{ji}}
     \end{aligned}\)

     so \(\mat A\mat B + \mat B\mat A\) is hermitian.
    \item
     \begin{itemize}
      \item \(
       \begin{aligned}[t]
        i(\mat A\mat B - \mat B\mat A)_{ij}
         &= i( A_{ik}  B_{kj} -  B_{ik}  A_{kj}) \\
         &= \conj{-i}(\conj{( A_{ki}  B_{jk}
           -  B_{ki}  A_{jk})}) \\
         &= \conj{-i}(\conj{( A_{ki}  B_{jk}
           -  B_{ki}  A_{jk})}) \\
         &= \conj{i( A_{jk}  B_{ki} -  B_{jk}  A_{ki})} \\
         &= \conj{i(\mat A\mat B - \mat B\mat A)_{ji}}
       \end{aligned}\)

       so \(i(\mat A\mat B - \mat B\mat A)\) is hermitian.
      \item
       For any scalar \(\lambda\) and square matrices \(\mat A\), \(\mat B\), we
       have that
       \begin{equation*}
        \tr(\mat A + \mat B)
         = (\mat A + \mat B)_{ii}
         =  A_{ii} +  B_{ii}
         = \tr\mat A + \tr\mat B
       \end{equation*}
       and
       \begin{equation*}
        \tr(\mat A\mat B)
        = (\mat A\mat B)_{ii}
        =  A_{ij}  B_{ji}
        =  B_{ji}  A_{ij}
        = (\mat B\mat A)_{jj}
         = \tr(\mat B\mat A)
       \end{equation*}
       and
       \begin{equation*}
        \tr(\lambda \mat A)
        = (\lambda \mat A)_{ii}
        = \lambda  A_{ii}
        = \lambda \tr\mat A
       \end{equation*}
       so then
       \begin{equation*}
        \tr(i(\mat A\mat B - \mat B\mat A))
        = i(\tr(\mat A\mat B) - \tr(\mat B\mat A))
        = i(\tr(\mat A\mat B) - \tr(\mat A\mat B)) = 0
       \end{equation*}
     \end{itemize}
    \item \(
      \tr(\mat A\mat B)
      = (\mat A\mat B)_{ii}
      =  A_{ij}  B_{ji}
      = \conj{ A_{ji}  B_{ij}}
      = \conj{(\mat A\mat B)_{jj}} = \conj{\tr(\mat A\mat B)}
      \)

      so \(\tr(\mat A\mat B)\) is real.

      \(\det(\mat A\mat B) = \det\mat A\det\mat B\), which is the product of
      real numbers, so is real.
     \item Using the earlier commutativity property,
      \(\tr(\mat U\mat A\,\herm{\mat U})
        = \tr(\mat A\,\herm{\mat U}\mat U) = \tr(\mat A\mat I) = \tr\mat A\).

      Using the multiplicativity of determinants,
      \(\det(\mat U\mat A\,\herm{\mat U})
        = \det\mat U\det\mat A\det(\herm{\mat U})
        = \det\mat U\det(\herm{\mat U})\det\mat A
        = \det(\mat U\herm{\mat U})\det\mat A
        = \det\mat I \det\mat A
        = \det \mat A
        \)
   \end{enumerate}
  \item
   \begin{alignat*} 2
    && \det \mat M
     &= \begin{vmatrix}
      a & a^2 & bc \\
      b & b^2 & ca \\
      c & c^2 & ab
     \end{vmatrix} \\
    \parens*{
     \begin{aligned}
      \vec r(2) &\to \vec r(2) - \vec r(1) \\
      \vec r(3) &\to \vec r(3) - \vec r(1)
     \end{aligned}} \quad
    && &= \begin{vmatrix}
      a & a^2 & bc \\
      b - a & b^2 - a^2 & ca - bc \\
      c - a & c^2 - a^2 & ab - bc
     \end{vmatrix} \\
    && &= (b - a)(c - a)\begin{vmatrix}
      a & a^2 & bc \\
      1 & b + a & -c \\
      1 & c + a & -b
     \end{vmatrix} \\
    \parens*{
     \begin{aligned}
      \vec r(1) &\to \vec r(1) - a\vec r(2) \\
      \vec r(3) &\to \vec r(3) - \vec r(2)
     \end{aligned}} \quad
    && &= (b - a)(c - a)\begin{vmatrix}
      0 & -ab & bc + ac \\
      1 & b + a & -c \\
      0 & c - b & -b + c
     \end{vmatrix} \\
    && &= (b - a)(c - a)(c - b)\begin{vmatrix}
      0 & -ab & bc + ac \\
      1 & b + a & -c \\
      0 & 1 & 1
     \end{vmatrix} \\
    (\text{expanding along the first column})
    && &= (a - b)(b - c)(c - a)\begin{vmatrix}
      1 & 1 \\
      -ab & bc + ac
     \end{vmatrix} \\
    && &= (a - b)(b - c)(c - a)(ab + bc + ac)
   \end{alignat*}
  \item
   \begin{lemma}[Rule of Sarrus]
    \label{lemma_sarrus}
    The determinant of a \(3 \times 3\) matrix \(\mat M\) is given by
    \begin{align*}
     \begin{vmatrix}
      \color{red}m_{11} & \color{violet}m_{12} & \color{blue}m_{13} \\
      \color{blue}m_{21} & \color{red}m_{22} & \color{violet}m_{23} \\
      \color{violet}m_{31} & \color{blue}m_{32} & \color{red}m_{33}
     \end{vmatrix} =
     \begin{vmatrix}
      \color{teal}m_{11} & \color{olive}m_{12} & \color{orange}m_{13} \\
      \color{olive}m_{21} & \color{orange}m_{22} & \color{teal}m_{23} \\
      \color{orange}m_{31} & \color{teal}m_{32} & \color{olive}m_{33}
     \end{vmatrix} &=
     \begin{aligned}[t]
      &  \textcolor{red}{m_{11} m_{22} m_{33}}
       + \textcolor{violet}{m_{12} m_{23} m_{31}}
       + \textcolor{blue}{m_{13} m_{21} m_{32}} \\
      &-(\textcolor{teal}{m_{11} m_{23} m_{32}}
       + \textcolor{olive}{m_{12} m_{21} m_{33}}
       + \textcolor{orange}{m_{13} m_{22} m_{31}})
     \end{aligned}
    \end{align*}
    This can be read as ``the sum of the leading diagonals minus the sum of the
    trailing diagonals'', considering diagonals wrapping around as if the matrix
    was on the surface of a torus.
   \end{lemma}
   \begin{proof}
    The \(6\) permutations in \(S_3\) are, in cycle notation, classified by
    parity:
    \begin{center}
     \begin{tabular}{*4c}
      \bfseries Even & \(\Id\) & \((1\ 3\ 2)\) & \((1\ 2\ 3)\) \\
      \bfseries Odd & \((2\ 3)\) & \((1\ 2)\) & \((1\ 3)\)
     \end{tabular}
    \end{center}
    as any cycle of odd length is an even permutation.

    In the Leibniz definition of the determinant, the even permutations
    then give the positive terms
    \({m_{11} m_{22} m_{33}}\),
    \({m_{12} m_{23} m_{31}}\),
    \({m_{13} m_{21} m_{32}}\).
    The odd permutations give the negative terms
    \(m_{11} m_{23} m_{32}\),
    \(m_{12} m_{21} m_{33}\),
    \(m_{13} m_{22} m_{31}\). So this is indeed a correct formula.
   \end{proof}
   So
   \begin{align*}
    \Delta(x, y, z) =
    \begin{vmatrix}
     x & y & z \\
     z & x & y \\
     y & z & x
    \end{vmatrix}
     &= x^3 + y^3 + z^3 - (xyz + yzx + zxy) \\
     &= x^3 + y^3 + z^3 - 3xyz
   \end{align*}
   But also,
   \begin{alignat*}2
    && \Delta(x, y, z) &= \begin{vmatrix}
     x & y & z \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
    \parens*{
     \vec r(1) \to \vec r(1) + \vec r(2) + \vec r(3)
    } \quad
    && &= \begin{vmatrix}
     x + y + z & x + y + z & x + y + z \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
    && &= (x + y + z)\begin{vmatrix}
     1 & 1 & 1 \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
    && \Delta(x, y, z) &= \begin{vmatrix}
     x & y & z \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
    \parens*{
     \vec c(1) \to \vec c(1) + \omega \vec c(2) + \omega^2 \vec c(3)
    } \quad
    && &= \begin{vmatrix}
     x + \omega y + \omega^2 z & y & z \\
     z + \omega x + \omega^2 y & x & y \\
     y + \omega z + \omega^2 x & z & x
    \end{vmatrix} \\
    && &= (x + \omega y + \omega^2 z)\begin{vmatrix}
     1 & y & z \\
     \omega & x & y \\
     \omega^2 & z & x
    \end{vmatrix} \\
    && \Delta(x, y, z) &= \begin{vmatrix}
     x & y & z \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
    \parens*{
     \vec c(1) \to \vec c(1) + \omega^2 \vec c(2) + \omega \vec c(3)
    } \quad
    && &= \begin{vmatrix}
     x + \omega^2 y + \omega z & y & z \\
     z + \omega^2 x + \omega y & x & y \\
     y + \omega^2 z + \omega x & z & x
    \end{vmatrix} \\
    && &= (x + \omega^2 y + \omega^2 z)\begin{vmatrix}
     1 & y & z \\
     \omega^2 & x & y \\
     \omega & z & x
    \end{vmatrix}
   \end{alignat*}
   So \((x + y + z)\), \((x + \omega y + \omega^2 z)\),
   \((x + \omega^2 y + \omega z)\) are all factors of
   \(\Delta(x, y, z)\). But since these multiply to give a coefficient of \(1\)
   in \(x^3\), they cannot be multiplied by any further factors, so
   \(\Delta(x, y, z)\) must be the product of these three factors.
  \item
   Using that for any scalar \(\lambda\) and any \(n \times n\) matrix
   \(\mat M\), \(\det\mat M = \det(\tran{\mat M})\) and
   \(\det(\lambda \mat M) = \lambda^n \det\mat M\), we have that
   \(\det\mat A = \det(\tran{\mat A})
     = \det(-\mat A) = (-1)^{2n + 1}\det\mat A = -\det\mat A\), so
   \(\det\mat A\) must be \(0\).
  \item
   Let \(\mat D_i\) denote the \(i \times i\) matrix of this form, so
   \(\mat D = \mat D_n\), and claim that
   \begin{equation*}
    \det  \mat D_n = (p + n - 1)(p - 1)^{n - 1}
   \end{equation*}
   The base case \(n = 1\) is true:
   \((p + 1 - 1)(p - 1)^0 = p = \det \mat D_1\).

   Also,
   \begin{alignat*}2
    && \det \mat D_n
    &= \begin{vmatrix}
     p & 1 & 1 & 1 & \cdots \\
     1 & p & 1 & 1 & \cdots \\
     1 & 1 & p & 1 & \cdots \\
     \vdots & \vdots & \vdots & \vdots & \ddots
    \end{vmatrix} \\
    \parens*{
     \vec r(i) \to \vec r(i) - \vec r(1), i \ne 1
    }
    && &= \begin{vmatrix}
     p & 1 & 1 & 1 & \cdots \\
     1 - p & p - 1 & 0 & 0 & \cdots \\
     1 - p & 0 & p - 1 & 0 & \cdots \\
     1 - p & 0 & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \vdots & \ddots
    \end{vmatrix} \\
    % this is a bit dirty
    \parens*{
     \begin{gathered}
      \text{Expanding along} \\
      \text{the second row}
     \end{gathered}} \quad
    && &= -(1 - p)\underbrace{\begin{vmatrix}
     1 & 1 & 1 & \cdots \\
     0 & p - 1 & 0 & \cdots \\
     0 & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \ddots \\
    \end{vmatrix}}_{(n - 1) \times (n - 1)} \\
    && &\phantom{={}} + (p - 1)\underbrace{\begin{vmatrix}
     p & 1 & 1 & \cdots \\
     1 - p & p - 1 & 0 & \cdots \\
     1 - p & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \ddots
    \end{vmatrix}}_{(n - 1) \times (n - 1)} \\
    \parens*{
     \begin{gathered}
      \text{Expanding along} \\
      \text{the first row}
     \end{gathered}} \quad
    && &= -(1 - p)\underbrace{\begin{vmatrix}
     p - 1 & 0 & 0 & \cdots \\
     0 & p - 1 & 0 & \cdots \\
     0 & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \ddots \\
    \end{vmatrix}}_{(n - 2) \times (n - 2)} \\
    \parens*{
     \vec r(i) \to \vec r(i) + \vec r(1), i \ne 1
    }
    && &\phantom{={}} + (p - 1)\underbrace{\begin{vmatrix}
     p & 1 & 1 & \cdots \\
     1 & p & 1 & \cdots \\
     1 & 1 & p & \cdots \\
     \vdots & \vdots & \vdots & \ddots
    \end{vmatrix}}_{(n - 1) \times (n - 1)} \\
    && &= (p - 1)^{n - 1} + (p - 1)\det  \mat D_{n - 1} \\
    (\text{by induction hypothesis})\quad
    && &= (p - 1)^{n - 1} + (p - 1) (p + n - 2)(p - 1)^{n - 2} \\
    && &= (p - 1)^{n - 1} + (p + n - 2)(p - 1)^{n - 1} \\
    && &= (p + n - 1)(p - 1)^{n - 1}
   \end{alignat*}
   So we are done, by induction.
  \item
   We present the cofactors as a cofactor matrix \(\mat \Delta\), with entries
   \(\mat \Delta_{ij}\).
   \begin{align*}
    \mat \Delta
    &= \begin{pmatrix*}[r]
     \begin{vmatrix*}[r]
      2 & 3 \\
      -2 & 2
     \end{vmatrix*} &
     -\begin{vmatrix}
      1 & 3 \\
      3 & 2
     \end{vmatrix} &
     \begin{vmatrix*}[r]
      1 & 2 \\
      3 & -2
     \end{vmatrix*} \\[3ex]
     -\begin{vmatrix}
      1 & 1 \\
      -2 & 2
     \end{vmatrix} &
     \begin{vmatrix}
      1 & 1 \\
      3 & 2
     \end{vmatrix} &
     -\begin{vmatrix*}[r]
      1 & 1 \\
      3 & -2
     \end{vmatrix*} \\[3ex]
     \begin{vmatrix}
      1 & 1 \\
      2 & 3
     \end{vmatrix} &
     -\begin{vmatrix}
      1 & 1 \\
      1 & 3
     \end{vmatrix} &
     \begin{vmatrix}
      1 & 1 \\
      1 & 2
     \end{vmatrix}
    \end{pmatrix*} \\
    &= \begin{pmatrix*}[r]
     10 & 7 & -8 \\
     -4 & -1 & 5 \\
     1 & -2 & 1
    \end{pmatrix*}
   \end{align*}
   Now the condition \( A_{ij}\mat  \Delta_{ik} = \delta_{jk} \det \mat A\) is
   equivalent to \((\tran{\mat \Delta})_{ki}  A_{ij} = \delta_{jk} \det \mat A\),
   ie \(\tran{\mat \Delta}\mat A = (\det \mat A)\mat I\). So let's calculate:
   \begin{equation*}
    \tran{\mat \Delta}\mat A =
    \begin{pmatrix*}[r]
     10 & -4 & 1 \\
     7 & -1 & -2 \\
     -8 & 5 & 1
    \end{pmatrix*}
    \begin{pmatrix*}[r]
     1 & 1 & 1 \\
     1 & 2 & 3 \\
     3 & -2 & 2
    \end{pmatrix*}
    = \begin{pmatrix}
     9 & 0 & 0 \\
     0 & 9 & 0 \\
     0 & 0 & 9
    \end{pmatrix}
    = 9\mat I
   \end{equation*}
   Using Lemma \ref{lemma_sarrus}, we can also calculate that indeed,
   \(\det \mat A = 4 + 9 - 2 - (6 - 6 + 2) = 9\).

   So
   \begin{equation*}
    \mat A^{-1}
     = \frac 1{\det \mat A} \tran{\mat \Delta}
     = \frac 19
     \begin{pmatrix*}[r]
      10 & -4 & 1 \\
      7 & -1 & -2 \\
      -8 & 5 & 1
     \end{pmatrix*}
   \end{equation*}
   Then
   \begin{alignat*}2
    && x + y + z &= 1 \\
    && x + 2y + 3z &= -5 \\
    && 3x - 2y + 2z &= 4 \\
    \iff{}&&
     \mat A\begin{pmatrix} x \\ y \\ z \end{pmatrix}
      &= \begin{pmatrix*}[r] 1 \\ -5 \\ 4 \end{pmatrix*} \\
    \iff{}&&
     \begin{pmatrix} x \\ y \\ z \end{pmatrix}
      &= \mat A^{-1}\begin{pmatrix*}[r] 1 \\ -5 \\ 4 \end{pmatrix*} \\
    && &=
    \frac 19
    \begin{pmatrix*}[r]
     10 & -4 & 1 \\
     7 & -1 & -2 \\
     -8 & 5 & 1
    \end{pmatrix*}
    \begin{pmatrix*}[r] 1 \\ -5 \\ 4 \end{pmatrix*} \\
    && &=
    \frac 19 \begin{pmatrix*}[r] 34 \\ 4 \\ -29 \end{pmatrix*}
   \end{alignat*}
   and we can check that
   \begin{align*}
    x + y + z &= \tfrac 19(34 + 4 - 29) = \tfrac 19(9) = 1 \\
    x + 2y + 3z &= \tfrac 19(34 + 8 - 87) = \tfrac 19(-45) = -5 \\
    3x - 2y + 2z &= \tfrac 19(102 - 8 - 58) = \tfrac 19(36) = 4
   \end{align*}
  \item \(
   \begin{alignedat}[t]2
    && x + y + z &= t \\
    && tx + 2z &= 3 \\
    && 3x + ty + 5z &= 7 \\
    \iff{}&&
    \begin{pmatrix}
     1 & 1 & 1 \\
     t & 0 & 2 \\
     3 & t & 5
    \end{pmatrix}
    \begin{pmatrix} x \\ y \\ z \end{pmatrix}
     &= \begin{pmatrix} t \\ 3 \\ 7 \end{pmatrix}
   \end{alignedat}\)

   but, using Lemma \ref{lemma_sarrus},
   \begin{equation*}
    \begin{vmatrix}
     1 & 1 & 1 \\
     t & 0 & 2 \\
     3 & t & 5
    \end{vmatrix}
    = 6 + t^2 - 5t - 2t
    = t^2 - 7t + 6
    = (t - 6)(t - 1)
   \end{equation*}
   so this matrix is invertible (ie there is a unique solution) if
   \(t \notin \set{1, 6}\), in which case the solution can be found by some
   elimination, safe in the knowledge that we can divide through by \((t - 1)\)
   and \((t - 6)\):
   \begin{alignat*}2
    && z &= \tfrac 12(3 - tx) \\
    \implies{}&& (1 - \tfrac 12 t)x + y &= t - \tfrac 32 \\
    && (3 - \tfrac 52 t) x + ty &= 7 - \tfrac{15}2 = -\tfrac 12 \\
    \implies{}&& y &= t - \tfrac 32 - (1 - \tfrac 12 t)x \\
    \implies{}&& (3 - \tfrac 52 t) x + t^2 - \tfrac 32 t - (t - \tfrac 12 t^2) x
     &= -\tfrac 12 \\
    \implies{}&& \tfrac 12(6 - 7t + t^2) x &= \tfrac 12(-2t^2 + 3t - 1) \\
    \implies{}&& (t - 6)(t - 1) x &= (1 - 2t)(t - 1) \\
    \implies{}&& x &= \frac{1 - 2t}{t - 6} \\
    \implies{}&& z &= \tfrac 12(3 - tx) \\
    && &= \frac{3t - 18 - t + 2t^2}{2(t - 6)} \\
    && &= \frac{t^2 + t - 9}{t - 6} \\
    \implies{}&& y &= t - x - z \\
    && &= \frac{t^2 - 6t - 1 + 2t - t^2 - t + 9}{t - 6} \\
    && &= \frac{-5t + 8}{t - 6}
   \end{alignat*}
   If, however, \(t = 1\) or \(t = 6\) then there is more work to be done.
   \begin{itemize}
     %TODO spell out operations
    \item If \(t = 1\):
     \begin{alignat*}2
      &&\begin{pmatrix}
       1 & 1 & 1 \\
       1 & 0 & 2 \\
       3 & 1 & 5
      \end{pmatrix}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 3 \\ 7 \end{pmatrix} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -1 & 1 \\
       0 & -2 & 2
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 2 \\ 4 \end{pmatrix} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -1 & 1 \\
       0 & 0 & 0
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}
     \end{alignat*}
     which is consistent, and is solved by letting \(z = \lambda\) be arbitrary,
     letting \(y = \lambda - 2\), and \(x = 1 - y - z = 3 - 2\lambda \). This is
     the line \(\vec x = (3, -2, 0) + \lambda(-2, 1, 1)\).
    \item If \(t = 6\):
     \begin{alignat*}2
      &&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       6 & 0 & 2 \\
       3 & 6 & 5
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 3 \\ 7 \end{pmatrix} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -6 & -4 \\
       0 & 3 & 2
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix*}[r] 1 \\ -3 \\ 4 \end{pmatrix*} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -6 & -4 \\
       0 & 0 & 0
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ -3 \\ 4 - \tfrac 32 \end{pmatrix}
     \end{alignat*}
     which is inconsistent, as \(4 \ne \tfrac 32\). So there are no solutions.
   \end{itemize}
  \item
   There exist solutions to \(\mat M \vec x = \vec d\) if and only if
   \(\vec d \in \Img \mat M\) (as this is simply the condition that there exist
   vectors which map to \(\vec d\)) , and then they are precisely the vectors of
   the form \(\vec x = \vec x_0 + \vec v\), where \(\vec v \in \Ker \mat M\),
   and \(\vec x_0\) is some particular solution vector such that
   \(\mat M \vec x_0 = \vec d\)
   (which we know must exist if \(\vec d \in \Img \mat M\)).

   These are all solutions as then
   \(\mat M(\vec x_0 + \vec v)
     = \mat M \vec x_0 + \mat M \vec v
     = \vec d + \vec 0 = \vec d\), and furthermore if some
   \(\vec x\) is a solution, then
   \(\mat M(\vec x - \vec x_0)
     = \mat M \vec x - \mat M \vec x_0
     = \vec d - \vec d
     = \vec 0\)
   so indeed \(\vec x - \vec x_0 \in \Ker \mat M\).

   Let's first calculate the determinant of \(\mat M\) in this case:
   \begin{alignat*}2
    &&\det \mat M &=
    \begin{vmatrix}
     1 & 1 & 1 \\
     1 & a & b \\
     1 & a^2 & b^2
    \end{vmatrix} \\
    \parens*{
     \vec c(2) \to \vec c(2) - \vec c(3)
    } \quad
    && &= \begin{vmatrix}
     1 & 0 & 1 \\
     1 & a - b & b \\
     1 & a^2 - b^2 & b^2
    \end{vmatrix} \\
    && &= (a - b)\begin{vmatrix}
     1 & 0 & 1 \\
     1 & 1 & b \\
     1 & a + b & b^2
    \end{vmatrix} \\
    \parens*{
     \vec c(3) \to \vec c(3) - \vec c(1)
    } \quad
    && &= (a - b)\begin{vmatrix}
     1 & 0 & 0 \\
     1 & 1 & b - 1 \\
     1 & a + b & b^2 - 1
    \end{vmatrix} \\
    && &= (a - b)(b - 1)\begin{vmatrix}
     1 & 0 & 0 \\
     1 & 1 & 1 \\
     1 & a + b & b + 1
    \end{vmatrix} \\
    (\text{expanding along the first row})
    && &= (a - b)(b - 1)\begin{vmatrix}
     1 & 1 \\
     a + b & b + 1
    \end{vmatrix} \\
    && &= (a - b)(b - 1)(1 - a)
   \end{alignat*}
   So:
   \begin{itemize}
    \item
     If \(a \ne b\) and \(a, b \ne 1\), then the image of \(\mat M\) is
     \(\Reals^3\), as the the determinant is nonzero so the columns are linearly
     independent. By the rank-nullity theorem, the kernel of \(\mat M\) is then
     the trivial kernel \(\set{\vec 0}\).
    \item
     If \(a = b\),
     \begin{itemize}
      \item
       If \(a = b = 1\), then \(\Img \mat M\) is simply the span of
       \((1, 1, 1)\), ie the line \(\vec x = \lambda(1, 1, 1)\).

       The kernel of \(\mat M\) is the plane \(\vec x \vecdot (1, 1, 1) = 0\).
      \item
       If \(a, b \ne 1\), then \(\Img \mat M\) is the span of
       \((1, 1, 1)\) and \((1, a, a^2)\), which is the plane
       \begin{alignat*}2
        && \vec x \vecdot ((1, 1, 1) \veccross (1, a, a^2)) &= 0 \\
        \iff{}&& \vec x \vecdot (a^2 - a, 1 - a^2, a - 1) &= 0 \\
        \iff{}&& \vec x \vecdot (a, -(a + 1), 1) &= 0
       \end{alignat*}
       The kernel of \(\mat M\) must be the intersection of the planes
       \(\vec x \vecdot (1, 1, 1) = 0\) and \(\vec x \vecdot(1, a, a) = 0\),
       which is the line
       \begin{alignat*}2
        && \vec x &= \lambda ((1, 1, 1) \veccross (1, a, a)) \\
        \iff{}&& \vec x &= \lambda (0, 1 - a, a - 1) \\
        \iff{}&& \vec x &= \lambda' (0, 1, -1)
       \end{alignat*}
     \end{itemize}
    \item
     If \(a \ne b\) but \(b = 1\), the image is the span of
     \((1, 1, 1)\) and \((1, a, a^2)\), which are linearly independent, so this
     is the plane \(\vec x \vecdot (a, -(a + 1), 1) = 0\), as in the above case
     where \(a = b\).

     The kernel is now the intersection of the planes
     \(\vec x \vecdot (1, 1, 1) = 0\) and \(\vec x \vecdot (1, a, 1) = 0\),
     which is the line
     \begin{alignat*}2
      &&\vec x &= \lambda((1, 1, 1) \veccross (1, a, 1)) \\
      \iff{}&&\vec x &= \lambda(1 - a, 0, a - 1) \\
      \iff{}&&\vec x &= \lambda'(1, 0, -1)
     \end{alignat*}
    \item
      If \(a \ne b\) but \(a = 1\), the image is turns out exactly the same as
      in the previous case: the plane
      \(\vec x \vecdot (b, -(b + 1), 1) = 0\).

      The kernel is now the intersection of the planes
      \(\vec x \vecdot(1, 1, 1) = 0\) and \(\vec x \vecdot (1, 1, b) = 0\),
      which is the line
     \begin{alignat*}2
      &&\vec x &= \lambda((1, 1, 1) \veccross (1, 1, b)) \\
      \iff{}&&\vec x &= \lambda(b - 1, 1, - b, 0) \\
      \iff{}&&\vec x &= \lambda'(1, -1, 0)
     \end{alignat*}
   \end{itemize}
   \begin{enumerate}[label=(\roman*)]
    \item
     As noted earlier, \(\mat M\) is non-singular precisely if
     \(a \ne b\) and \(a, b \ne 1\), in which case there is a unique solution.
    \item \label{item_ii}
     For there to be more than one solution, we need \(\mat M\) to be singular
     (so that its kernel is nontrivial) and \(\vec d \in \Img \mat M\).

     Revisiting each case,
     \begin{itemize}
      \item
       If \(a = b\),
       \begin{itemize}
        \item
         If \(a = b = 1\), there is no solution, as
         \(\vec d\) is not of the form \(\lambda(1, 1, 1)\).
        \item
         If \(a, b \ne 1\), then
         \begin{alignat*}2
          &&\vec d &\in \Img \mat M \\
          \iff{}&& \vec d \vecdot (a, -(a + 1), 1) &= 0 \\
          \iff{}&& 2a + 2 &= 0 \\
          \iff{}&& a &= -1
         \end{alignat*}
         so \textbf{there are solutions} if
         \(a = b = -1\). The solutions are then the line
         \(\vec x = (0, 1, 0) + \lambda(0, 1, -1)\).
       \end{itemize}
      \item
       If \(a \ne b\) but \(b = 1\), then the image remains the same,
       so \textbf{there are solutions} if \(a = -1\) and \(b = 1\), and the
       solutions are then the line
       \(\vec x = (0, 1, 0) + \lambda(1, 0, -1)\)
      \item
       Similarly, \textbf{there are solutions} if
       \(a = 1\) and \(b = -1\), given by the line
       \(\vec x = (0, 0, 1) + \lambda(1, -1, 0)\).
     \end{itemize}
    \item
     This is the complement to part \ref{item_ii}. There are no solutions if
     \(a\) and \(b\) are not both either \(1\) or \(-1\), or both are \(1\).
   \end{enumerate}
  \item
   \begin{enumerate}[label=(\alph*)]
    \item
     We claim that the rotation matrix
     \begin{equation*}
      \mat R =
      \begin{pmatrix*}[r]
       0 & -1 & 0 \\
       1 & 0 & 0 \\
       0 & 0 & 1 \\
      \end{pmatrix*}
     \end{equation*}
     has eigenvalues \(1, \pm i\). If you happen to have been told in lectures
     that the eigenvalues of a \(2 \times 2\) rotation matrix by an angle
     \(\theta\) are \(e^{\pm i\theta}\), then this is fairly obvious. If in
     doubt, we can always use Lemma \ref{lemma_sarrus} to find and solve the
     characteristic polynomial:
     \begin{align*}
      \chi_{\mat R}(t)
       &= \det(\mat R - t\mat I) \\
       &= \begin{vmatrix}
        -t & -1 & 0 \\
        1 & -t & 0 \\
        0 & 0 & 1 - t
       \end{vmatrix} \\
       &= t^2(1 - t) + (1 - t) \\
       &= (t^2 + 1)(1 - t) \\
       &= (t + i)(t - i)(1 - t)
     \end{align*}
     and we are done.
    \item
     We claim that the matrix
     \begin{equation*}
      \mat B =
      \begin{pmatrix}
       0 & 1 & 0 \\
       0 & 0 & 0 \\
       0 & 0 & 0
      \end{pmatrix}
     \end{equation*}
     has all three eigenvalues zero. Let's use Lemma \ref{lemma_sarrus} to find
     its characteristic polynomial:
     \begin{align*}
      \chi_{\mat B}(t)
       &= \det(\mat B - t \mat I) \\
       &= \begin{vmatrix}
        -t & 1 & 0 \\
        0 & -t & 0 \\
        0 & 0 & -t
       \end{vmatrix} \\
       &= -t^3
     \end{align*}
     It has been cleverly constructed so that every diagonal other than the
     leading one of \(\mat B - t \mat I\) has a zero, resulting in a
     characteristic polynomial of \(-t^3\), which has all three roots \(0\).
    \item
     Let \(\lambda\) be an arbitrary eigenvalue of \(\mat A\). Then, by
     definition, for the corresponding eigenvector \(\vec v \ne \vec 0\), we
     have \(\mat A \vec v = \lambda \vec v\). But then \footnote{by induction}
     \(\mat A^m \vec v = \lambda^m \vec v\), while also
     \(\mat A^m \vec v = \mat 0 \vec v = \vec 0\). But
     \(\vec v \ne \vec 0\), so \(\lambda^m = 0\), so \(\lambda = 0\).

     So all the eigenvalues of \(\mat A\) are \(0\).
    \item
     Let
     \begin{equation*}
      \mat M = \begin{pmatrix}
       m_{11} & m_{12} \\
       m_{21} & m_{22}
      \end{pmatrix},
     \end{equation*}
     so
     \begin{align*}
      \chi_{\mat M}(t) &= \det(\mat M - t \mat I) \\
       &= \begin{vmatrix}
       m_{11} - t & m_{12} \\
       m_{21} & m_{22} - t
      \end{vmatrix} \\
       &= (m_{11} - t)(m_{22} - t) - m_{12} m_{21} \\
       &= t^2 - (m_{11} + m_{22}) t + m_{11} m_{22} - m_{12} m_{21}
     \end{align*}
     This is a quadratic with real coefficients. If it is to have strictly
     non-real eigenvalues, then its discriminant must be strictly negative. ie
     \begin{alignat*}2
      && (m_{11} + m_{22})^2 - 4(m_{11} m_{22} - m_{12} m_{21}) &< 0 \\
      \iff{}&& (m_{11} - m_{22})^2 + 4m_{12} m_{21} &< 0 \\
      \iff{}&& (m_{11} - m_{22})^2 &< -4m_{12} m_{21}
     \end{alignat*}
     But \((m_{11} - m_{22})^2 \ge 0\), so \(-4 m_{12} m_{21} > 0\), and
     therefore \(-4 m_{12} m_{21} \ne 0\), and hence
     \(m_{12} \ne 0\) and \(m_{21} \ne 0\).
     The diagonal elements can be zero, for example in our old friend the
     rotation matrix:
     \begin{equation*}
      \mat R = \begin{pmatrix*}[r]
       0 & - 1 \\
       1 & 0
      \end{pmatrix*},
     \end{equation*}
     having eigenvalues \(\pm i\).
   \end{enumerate}
  \item
   First determine the characteristic equation:
   \begin{align*}
    \chi_{\mat A}(t)
     &= \det(\mat A - t\mat I) \\
     &= \begin{vmatrix}
      1 - t & \alpha & 0 \\
      \beta & 1 - t & 0 \\
      0 & 0 & 1 - t
     \end{vmatrix} \\
     &= (1 - t)\begin{vmatrix}
      1 - t & \alpha \\
      \beta & 1 - t
     \end{vmatrix},\quad (\text{by expanding along the third row}) \\
     &= (1 - t)((1 - t)^2 - \alpha \beta) \\
     &= (1 - t)(1 + \sqrt{\alpha \beta} - t)(1 - \sqrt{\alpha \beta} - t)
   \end{align*}
   where \(\sqrt{\alpha \beta}\) is the principal square root of
   \(\alpha \beta\) in \(\Complex\). So the eigenvalues of \(\mat A\) are
   \(1\), \(1 \pm \sqrt{\alpha \beta}\). Then to find the eigenvectors:
   \begin{alignat*}3
    \lambda_1 = 1:\quad&&
     \begin{pmatrix}
      0 & \alpha & 0 \\
      \beta & 0 & 0 \\
      0 & 0 & 0
     \end{pmatrix}
     \vec v_1 &= \vec 0
     &&\impliedby \vec v_1 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \\
    \lambda_2 = 1 + \sqrt{\alpha \beta}:\quad&&
     \begin{pmatrix}
      -\sqrt{\alpha \beta} & \alpha & 0 \\
      \beta & -\sqrt{\alpha \beta} & 0 \\
      0 & 0 & -\sqrt{\alpha \beta}
     \end{pmatrix}
     \vec v_2 &= \vec 0
     &&\impliedby \vec v_2 =
     \begin{pmatrix} \sqrt \alpha \\ \sqrt \beta \\ 0 \end{pmatrix} \\
    \lambda_3 = 1 - \sqrt{\alpha \beta}:\quad&&
     \begin{pmatrix}
      \sqrt{\alpha \beta} & \alpha & 0 \\
      \beta & \sqrt{\alpha \beta} & 0 \\
      0 & 0 & \sqrt{\alpha \beta}
     \end{pmatrix}
     \vec v_3 &= \vec 0
     &&\impliedby \vec v_3 =
     \begin{pmatrix} -\sqrt \alpha \\ \sqrt \beta \\ 0 \end{pmatrix} \\
   \end{alignat*}
   \begin{enumerate}[label=(\roman*)]
    \item
     The eigenvalues are real if
     \(\alpha \beta\) has a real square root, ie it is a non-negative real
     number.
    \item
     The eigenvector \(\vec v_1\) is always orthogonal to
     \(\vec v_2\) and \(\vec v_3\), so the only interesting case is the
     orthogonality of \(\vec v_2\) and \(\vec v_3\). These are orthogonal if:
     \begin{alignat*}2
      && \herm{\vec v_2} \vec v_3 &= 0 \\
      \iff{}&&
      - \conj{\sqrt \alpha} \sqrt \alpha
      + \conj{\sqrt \beta} \sqrt \beta &= 0 \\
      \iff{}&&
      \abs{\sqrt \alpha}^2 &= \abs{\sqrt \beta}^2 \\
      \iff{}&&
      \abs{\alpha} &= \abs{\beta}
     \end{alignat*}
   \end{enumerate}
   If both of these conditions hold, then we have that
   \(\alpha \beta = k\), for some positive real \(k\). But we also know that
   \(\abs{\alpha} = \abs{\beta}\), so \(k = \abs{\alpha}^2\). But then,
   \(\beta = k / \alpha = \abs{\alpha^2} / \alpha = \conj{\alpha}\).
   \(\mat A\) is then hermitian, as the only other elements are the (real)
   \(1\)s on the leading diagonal.

   For the reverse implication, if \(\mat A\) is hermitian, then
   \(\beta = \conj \alpha\), and therefore
   \(\alpha \beta = \abs{\alpha^2} \ge 0\)
   and \(\abs{\alpha} = \abs{\beta}\), so both conditions hold.
  \item
   If \(\mat Q\) is orthogonal, and \(\lambda\) is an eigenvalue of
   \(\mat Q\), with eigenvector \(\vec v \ne \vec 0\), then we can exploit the
   fact that orthogonal matrices preserve the inner product:
   \begin{alignat*}2
    &&\mat Q \vec v &= \lambda \vec v \\
    \implies{}&&
    \abs{\mat Q \vec v} &= \abs{\lambda \vec v} \\
    \implies{}&&
    \abs{\mat Q \vec v}^2 &= \abs{\lambda}^2 \abs{\vec v}^2 \\
    \implies{}&&
    \tran{(\mat Q \vec v)}(\mat Q \vec v) &= \abs{\lambda}^2 \abs{\vec v}^2 \\
    \implies{}&&
    \tran{\vec v}\tran{\mat Q}\mat Q \vec v &= \abs{\lambda}^2 \abs{\vec v}^2 \\
    \implies{}&&
    \tran{\vec v}\vec v &= \abs{\lambda}^2 \abs{\vec v}^2 \\
    \implies{}&&
    \abs{\vec v}^2 &= \abs{\lambda}^2 \abs{\vec v}^2 \\
    \implies{}&&
    1 &= \abs{\lambda}^2 \\
    \implies{}&&
    1 &= \abs{\lambda}
   \end{alignat*}
   So all eigenvalues have modulus \(1\). Now as \(\mat Q\) is
   \((2n + 1) \times (2n + 1)\), its characteristic polynomial has odd degree.
   It therefore has \emph{at least} one real root (for instancce because between
   very large negative and positive \(t\), there will be a change of sign).

   But since this eigenvalue has modulus \(1\), it is \(\pm 1\). Since the
   eigenvalues are roots of the (real) characteristic polynomial, they are in
   conjugate pairs (which each multiply to 1, as they all have modulus 1). Since
   the determinant of \(\mat Q\) is \(1\), the product of all the eigenvalues is
   \(1\). There must be an odd number of real eigenvalues, and they can't all be
   \(-1\), as then the product of all the eigenvalues would be \(-1\). So there
   is at least one eigenvalue of \(1\).

   When \(\mat Q\) is \(3 \times 3\), this can be interpreted as the fact that
   in \(3\) dimensions, any orthogonal transformation has an axis that is
   invariant under that transformation, ie every rotation is about an axis.

   If \(\det \mat Q = -1\), then by a similar argument \(\mat Q\) must have an
   eigenvalue of \(-1\) (which is interpreted as the existence of an axis that
   is inverted under that transformation (which is therefore a reflection)).
  \item
   \begin{enumerate}[label=(\textbf{\Alph*})]
    \item
     \begin{enumerate}[label=(\roman*)]
      \item Expanding with Lemma \ref{lemma_sarrus},
       \begin{align*}
        \chi_{\mat A}(t)
         &= \det(\mat A - t \mat I) \\
         &= \begin{vmatrix}
          5 - t & -3 & 2 \\
          6 & -4 - t & 4 \\
          4 & -4 & 5 - t
         \end{vmatrix} \\
        &= (5 - t)(-4 - t)(5 - t) - 48 - 48 \\
        &\phantom{={}} - (-18(5 - t) - 16(5 - t) + 8(-4 - t)) \\
        &= -(t + 4)(t^2 - 10t + 25) - 96 - (-202 + 26t) \\
        &= -t^3 + 10t^2 - 4t^2 - 25t + 40t - 26t - 100 - 96 + 202 \\
        &= -t^3 + 6t^2 - 11t + 6 \\
        &= (1 - t)(t^2 - 5t + 6) \\
        &= (1 - t)(t - 2)(t - 3)
       \end{align*}
       So the eigenvalues of \(\mat A\) are \(1\), \(2\), \(3\).
      \item
       The eigenspace \(E_{\lambda_i}\) of an eigenvalue
       \(\lambda_i\) is the kernel of \(\mat A - \lambda_i \mat I\). So:
       \begin{alignat*}3
        \lambda_1 = 1:\quad&&
        &&\begin{pmatrix}
         4 & -3 & 2 \\
         6 & -5 & 4 \\
         4 & -4 & 4
        \end{pmatrix}
        \vec v_1 &= \vec 0 \\
        \parens*{\vec r(3) \to \tfrac 14 \vec r(3)}\quad
        && \iff{}&&\begin{pmatrix}
         4 & -3 & 2 \\
         6 & -5 & 4 \\
         1 & -1 & 1
        \end{pmatrix}
        \vec v_1 &= \vec 0 \\
        \parens*{
         \begin{aligned}
          \vec r(1) &\to \vec r(1) - \vec r(3) \\
          \vec r(2) &\to \vec r(2) - \vec r(3) \\
         \end{aligned}
        }\quad
        && \iff{}&&\begin{pmatrix*}[r]
         0 & 1 & -2 \\
         0 & 1 & -2 \\
         1 & -1 & 1
        \end{pmatrix*}
        \vec v_1 &= \vec 0
       \end{alignat*}
       which is true iff \(\vec v_1\) lies in the intersection of the planes
       \(\vec x \vecdot \vec(0, 1, -2) = 0\) and
       \(\vec x \vecdot \vec(1, -1, 1) = 0\), which is a line, which by
       inspection is given for instance by
       \(E_{\lambda_1} = \set{\mu (1, 2, 1)}\). So
       \(\dim E_{\lambda_2} = 1\).

       Then
       \begin{alignat*}3
        \lambda_2 = 2:\quad&&
        &&\begin{pmatrix}
         3 & -3 & 2 \\
         6 & -6 & 4 \\
         4 & -4 & 3
        \end{pmatrix}
        \vec v_2 &= \vec 0 \\
        \parens*{
         \begin{aligned}
          \vec r(2) &\to \vec r(2) - 2\vec r(1) \\
          \vec r(3) &\to \vec r(3) - \tfrac 43\vec r(1)
         \end{aligned}
        }
        &&\iff{}&&\parens*{\begin{array}{crc}
         3 & -3 & 2 \\
         0 & 0 & 0 \\
         0 & 0 & 3 - \tfrac 83
        \end{array}}
        \vec v_2 &= \vec 0 \\
       \end{alignat*}
       which is true iff \(\vec v_2\) lies in the intersection of the planes
       \(\vec x \vecdot (3, -3, 2) = 0\) and
       \(\vec x \vecdot (0, 0, 1) = 0\). By inspection, this is the line
       \(E_{\lambda_2} = \set{\mu(1, 1, 0)}\). So also
       \(\dim E_{\lambda_2} = 1\).

       And lastly,
       \begin{alignat*}3
        \lambda_3 = 3:\quad&&
        &&\begin{pmatrix}
         2 & -3 & 2 \\
         6 & -7 & 4 \\
         4 & -4 & 2
        \end{pmatrix}
        \vec v_3 &= \vec 0 \\
        \parens*{
         \begin{aligned}
          \vec r(2) &\to \vec r(2) - 3 \vec r(1) \\
          \vec r(3) &\to \vec r(3) - 2 \vec r(1)
         \end{aligned}
        } \quad
        &&\iff{}&&\begin{pmatrix*}[r]
         2 & -3 & 2 \\
         0 & 2 & -2 \\
         0 & 2 & -2
        \end{pmatrix*}
        \vec v_3 &= \vec 0
       \end{alignat*}
       which is the intersection of the planes
       \(\vec x \vecdot (2, -3, 2) = 0\) and
       \(\vec x \vecdot (0, 1, -1) = 0\), which is by inspection
       \(E_{\lambda_3} = \set{\mu(1, 2, 2)}\). So also
       \(\dim E_{\lambda_3} = 1\).
      \item
       So \(\mat A\) is diagonalisable, as its eigenspaces have total dimension
       \(3\), so the eigenvectors of \(A\) are linearly independent, and are a
       basis spanning \(\Reals^3\).
     \end{enumerate}
    \item
     \begin{enumerate}[label=(\roman*)]
      \item Expanding with Lemma \ref{lemma_sarrus},
       \begin{align*}
        \chi_{\mat B}(t)
         &= \det(\mat B - t\mat I) \\
         &= \begin{vmatrix}
          1 - t & -3 & 4 \\
          4 & -7 - t & 8 \\
          6 & -7 & 7 - t
         \end{vmatrix} \\
         &= (1 - t)(-7 - t)(7 - t) - 112 - 144 \\
         &\phantom{={}} - (-12(7 - t) + 24(-7 - t) - 56(1 - t)) \\
         &= (1 - t)(t^2 - 49) - 256 - (-84 + 12t - 168 - 24t - 56 + 56t) \\
         &= -t^3 + t^2 + 49t - 49 - 256 - (-308 + 44t) \\
         &= -t^3 + t^2 + 5t + 3 \\
         &= (t + 1)(-t^2 + 2t + 3) \\
         &= -(t + 1)^2(t - 3)
       \end{align*}
       So the eigenvalues of \(\mat B\) are \(3\), and \(-1\) (with multiplicity
       \(2\)).
      \item
       Now to find the eigenspaces:
       \begin{alignat*}3
        \lambda_1 = 3:\quad
        && &&\begin{pmatrix*}[r]
         -2 & -3 & 4 \\
         4 & -10 & 8 \\
         6 & -7 & 4
        \end{pmatrix*}
        \vec v_1 &= \vec 0 \\
        \parens*{
         \begin{aligned}
          \vec r(2) &\to \vec r(2) + 2 \vec r(1) \\
          \vec r(3) &\to \vec r(3) + 3 \vec r(1) \\
         \end{aligned}
        } \quad
        &&\iff{}&&\begin{pmatrix*}[r]
         -2 & -3 & 4 \\
         0 & -16 & 16 \\
         0 & -16 & 16
        \end{pmatrix*}
        \vec v_1 &= \vec 0
       \end{alignat*}
       which is the intersection of the planes
       \(\vec x \vecdot (0, 1, -1) = 0\) and
       \(\vec x \vecdot (-2, -3, 4) = 0\), which is by inspection
       \(E_{\lambda_1} = \set{\mu(1, 2, 2)}\). So
       \(\dim E_{\lambda_1} = 1\).

       Then
       \begin{alignat*}3
        \lambda_2 = -1:\quad
        && &&\begin{pmatrix*}[r]
         2 & -3 & 4 \\
         4 & -6 & 8 \\
         6 & -7 & 8
        \end{pmatrix*}
        \vec v_2 &= \vec 0 \\
        \parens*{
         \begin{aligned}
          \vec r(2) &\to \vec r(2) - 2 \vec r(1) \\
          \vec r(3) &\to \vec r(3) - 3 \vec r(1)
         \end{aligned}
        } \quad
        &&\iff{}&&\begin{pmatrix*}[r]
         2 & -3 & 4 \\
         0 & 0 & 0 \\
         0 & -1 & 0
        \end{pmatrix*}
        \vec v_2 &= \vec 0 \\
       \end{alignat*}
       which is the intersection of the planes
       \(\vec x \vecdot (2, -3, 4) = 0\) and
       \(\vec x \vecdot (0, 1, 0) = 0\), which is by inspection the \emph{line}
       \(E_{\lambda_2} = \set{\mu(2, 0, -1)}\). So
       \(\dim E_{\lambda_2} = 1\).
      \item
       The sum of the dimensions of the eigenspaces of \(\mat B\) is \(2\), so
       the eigenvectors of \(\mat B\) cannot possibly form a basis for
       \(\Reals^3\). So \(\mat B\) is not diagonalisable.
     \end{enumerate}
    \item
     \begin{enumerate}[label=(\roman*)]
      \item
       Expanding with Lemma \ref{lemma_sarrus} and
       Listing \ref{listing_py},
       \begin{align*}
        \chi_{\mat C}(t)
         &= \begin{vmatrix}
          7 - t & -12 & 6 \\
          10 & -19 - t & 10 \\
          12 & -24 & 13 - t
         \end{vmatrix} \\
         &= (7 - t)(-19 - t)(13 - t) - 1440 - 1440 \\
         &\phantom{={}} - (-120(13 - t) + 72(-19 - t) - 240(7 - t) \\
         &= -t^3 + t^2 + t - 1 \\
         &= (t + 1)(-t^2 + 2t - 1) \\
         &= -(t + 1)(t - 1)^2
       \end{align*}
       So the eigenvalues of \(\mat C\) are \(-1\), and \(1\) (with multiplicity
       \(2\)).
       \begin{listing}[H]
        \inputminted{python}{char_poly.py}
        \caption{\texttt{char\_poly.py} \label{listing_py}}
       \end{listing}
      \item
       Now to find the eigenspaces:
       \begin{alignat*}3
        \lambda_1 = -1:\quad
        && &&\begin{pmatrix*}[r]
         8 & -12 & 6 \\
         10 & -18 & 10 \\
         12 & -24 & 14
        \end{pmatrix*}
        \vec v_1 &= \vec 0 \\
        \parens*{
         \begin{aligned}
          \vec r(1) &\to \tfrac 12 \vec r(1) \\
          \vec r(2) &\to \tfrac 12 \vec r(2) \\
          \vec r(3) &\to \tfrac 12 \vec r(3) \\
         \end{aligned}
        } \quad
        &&\iff{}&&\begin{pmatrix*}[r]
         4 & -6 & 3 \\
         5 & -9 & 5 \\
         6 & -12 & 7
        \end{pmatrix*}
        \vec v_1 &= \vec 0 \\
        \parens*{
         \vec r(3) \to \vec r(3) + \vec r(1) - 2\vec r(2)
        } \quad
        &&\iff{}&&\begin{pmatrix*}[r]
         4 & -6 & 3 \\
         5 & -9 & 5 \\
         0 & 0 & 0
        \end{pmatrix*}
        \vec v_1 &= \vec 0 \\
       \end{alignat*}
       This is the intersection of the planes
       \(\vec x \vecdot(4, -6, 3) = 0\) and
       \(\vec x \vecdot(5, -9, 5) = 0\). This is the line
       \(E_{\lambda_1}
         = \set{\mu (4, -6, 3) \veccross (5, -9, 5)}
         = \set{\mu (-3, -5, -6)}\). So
       \(\dim E_{\lambda_1} = 1\).

       Then
       \begin{alignat*}3
        \lambda_2 = 1:\quad
        && &&\begin{pmatrix*}[r]
         6 & -12 & 6 \\
         10 & -20 & 10 \\
         12 & -24 & 12
        \end{pmatrix*}
        \vec v_2 &= \vec 0 \\
        \parens*{
         \begin{aligned}
          \vec r(1) &\to \tfrac 16 \vec r(1) \\
          \vec r(2) &\to \tfrac 1{10} \vec r(2) \\
          \vec r(3) &\to \tfrac 1{12} \vec r(3)
         \end{aligned}
        } \quad
        &&\iff{}&&\begin{pmatrix*}[r]
         1 & -2 & 1 \\
         1 & -2 & 1 \\
         1 & -2 & 1
        \end{pmatrix*}
        \vec v_2 &= \vec 0 \\
       \end{alignat*}
       which is true iff \(\vec v_1\) lies in the plane
       \(\vec x \vecdot (1, -2, 1) = 0\), which can be written for instance as
       \(E_{\lambda_2} = \set{\mu(1, 1, 1) + \nu(2, 1, 0)}\). So
       \(\dim E_{\lambda_2} = 2\).
      \item
       The sum of the dimensions of the eigenspaces of \(\mat C\) is \(3\), so
       the eigenvectors of \(\mat C\) \emph{do} form a basis for
       \(\Reals^3\), and \(\mat C\) is diagonalisable.
     \end{enumerate}
   \end{enumerate}
  \item \(
   \begin{aligned}[t]
    \chi_{\mat A^{-1}}(t)
    &= \det(\mat A^{-1} - t\mat I) \\
    &= \det(-\mat A^{-1})\det(t\mat A - \mat I) \\
    &= \det(-\mat A^{-1})\det(t\mat A - \mat I) \\
    &= (-1)^n\det(\mat A^{-1})\det(t\mat A - \mat I) \\
    &= (-1)^n t^n \det(\mat A^{-1})\det(\mat A - \tfrac 1t \mat I) \\
    &= (-1)^n t^n \det(\mat A^{-1})\chi_{\mat A}(\tfrac 1t) \\
    &= (-1)^n t^n \det(\mat A^{-1})
       (a_0 + a_1 \tfrac 1t + a_2 (\tfrac 1t)^2 + \dotsb + a_n (\tfrac 1t)^n) \\
    &= (-1)^n \det(\mat A^{-1})
       (a_0 t^n + a_1 t^{n - 1} + a_2 t^{n - 2} + \dotsb + a_n) \\
    &= (-1)^n \det(\mat A^{-1})(a_n + a_{n - 1} t + \dotsb + a_0 t^n)
   \end{aligned} \)
 \end{enumerate}
\end{document}
