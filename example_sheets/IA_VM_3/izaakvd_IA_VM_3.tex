% Compiling with
% latexmk -halt-on-error -shell-escape -synctex=1 -pdf
% (Recommend using a latexmkrc file so as just to run latexmk -pvc, for example)
% Probably you can achieve the same with an inordinate number of invocations of
% pdflatex -halt-on-error -shell-escape -synctex=1

% fleqn aligns equations to the left, a4 paper size, 11pt font, article class
\documentclass[fleqn,a4paper,11pt]{article}
\title{IA Vectors and Matrices Example Sheet 3}
\author{Izaak van Dongen (\texttt{imv26})}

\usepackage{mymaths}
\usepackage{mystyle}

%TODO: insert \mat macros

\begin{document}
 \maketitle\thispagestyle{empty} % no page number under title

 \begin{enumerate}[label=\textbf{\arabic*.}]
  \item
   As \(A\) and \(B\) are hermitian, generally we have
   \(A_{ij} = \conj{A_{ji}}\), and
   \(B_{ij} = \conj{B_{ji}}\).
   \begin{enumerate}[label=(\roman*)]
    \item
     \begin{itemize}
      \item
       \(\tr A = A_{ii} = \conj{A_{ii}} = \conj{\tr A}\), as
       complex conjugation distributes over addition, and therefore
       \(\Im(\tr A) = \tfrac 12 (\tr A - \conj{\tr A}) = 0\), ie
       \(\tr A\) is real.
      \item
       Similarly,
       \begin{align*}
        \det A
         &= \sum_\sigma \epsilon(\sigma) \prod_i A_{i\,\sigma(i)} \\
         &= \sum_\sigma \epsilon(\sigma) \prod_i \conj{A_{\sigma(i)\,i}} \\
         &= \conj{\sum_\sigma \epsilon(\sigma) \prod_i A_{\sigma(i)\,i}} \\
         &= \conj{\sum_{\sigma^{-1}} \epsilon(\sigma^{-1})
                 \prod_i A_{i\,\sigma^{-1}(i)}} \\
         &= \conj{\det A},\quad
         \text{as the sum remains over all permutations \footnotemark}
       \end{align*}
       \footnotetext{
        ie if we let \(\tau = \sigma^{-1}\) then the sum is over all
        permutations \(\tau\), as inversion is a bijection from
        \(S_n \to S_n\).
       }
       so \(\det A\) must be real.
     \end{itemize}
    \item \(
     \begin{aligned}[t]
     (AB + BA)_{ij}
       &= A_{ik}B_{kj} + B_{ik}A_{kj} \\
       &= \conj{A_{ki}} \cdot \conj{B_{jk}} + \conj{B_{ki}} \cdot \conj{A_{jk}}
       = \conj{A_{ki}B_{jk} + B_{ki}A_{jk}} \\
       &= \conj{A_{jk}B_{ki} + B_{jk}A_{ki}} \\
       &= \conj{(AB + BA)_{ji}}
     \end{aligned}\)

     so \(AB + BA\) is hermitian.
    \item
     \begin{itemize}
      \item \(
       \begin{aligned}[t]
        i(AB - BA)_{ij}
         &= i(A_{ik} B_{kj} - B_{ik} A_{kj}) \\
         &= \conj{-i}(\conj{(A_{ki} B_{jk} - B_{ki} A_{jk})}) \\
         &= \conj{-i}(\conj{(A_{ki} B_{jk} - B_{ki} A_{jk})}) \\
         &= \conj{i(A_{jk} B_{ki} - B_{jk} A_{ki})} \\
         &= \conj{i(AB - BA)_{ji}}
       \end{aligned}\)

       so \(i(AB - BA)\) is hermitian.
      \item
       For any scalar \(\lambda\) and square matrices \(A\), \(B\), we have that
       \begin{equation*}
        \tr(A + B) = (A + B)_{ii} = A_{ii} + B_{ii} = \tr(A) + \tr(B)
       \end{equation*}
       and
       \begin{equation*}
        \tr(AB) = (AB)_{ii} = A_{ij} B_{ji} = B_{ji} A_{ij} = (BA)_{jj}
         = \tr(BA)
       \end{equation*}
       and
       \begin{equation*}
        \tr(\lambda A) = (\lambda A)_{ii} = \lambda A_{ii} = \lambda \tr(A)
       \end{equation*}
       so then
       \begin{equation*}
        \tr(i(AB - BA)) = i(\tr(AB) - \tr(BA)) = i(\tr(AB) - \tr(AB)) = 0
       \end{equation*}
     \end{itemize}
    \item \(
      \tr(AB) = (AB)_{ii} = A_{ij} B_{ji} = \conj{A_{ji} B_{ij}}
       = \conj{(AB)_{jj}} = \conj{\tr(AB)}
      \)

      so \(\tr(AB)\) is real.

      \(\det(AB) = \det(A)\det(B)\), which is the product of real numbers, so is
      real.
     \item Using the earlier commutativity property,
      \(\tr(UA\,\herm U) = \tr(A\,\herm UU) = \tr(AI) = \tr(A)\).

      Using the multiplicativity of determinants,
      \(\det(UA\,\herm U) = \det(U)\det(A)\det(\herm U)
        = \det(U)\det(\herm U)\det(A)
        = \det(U\herm U)\det(A)
        = \det(I) \det(A)
        = \det A
        \)
   \end{enumerate}
  \item
   \begin{alignat*} 2
    && \det M
     &= \begin{vmatrix}
      a & a^2 & bc \\
      b & b^2 & ca \\
      c & c^2 & ab
     \end{vmatrix} \\
    \parens*{
     \begin{aligned}
      \vec r(2) &\to \vec r(2) - \vec r(1) \\
      \vec r(3) &\to \vec r(3) - \vec r(1)
     \end{aligned}} \quad
    && &= \begin{vmatrix}
      a & a^2 & bc \\
      b - a & b^2 - a^2 & ca - bc \\
      c - a & c^2 - a^2 & ab - bc
     \end{vmatrix} \\
    && &= (b - a)(c - a)\begin{vmatrix}
      a & a^2 & bc \\
      1 & b + a & -c \\
      1 & c + a & -b
     \end{vmatrix} \\
    \parens*{
     \begin{aligned}
      \vec r(1) &\to \vec r(1) - a\vec r(2) \\
      \vec r(3) &\to \vec r(3) - \vec r(2)
     \end{aligned}} \quad
    && &= (b - a)(c - a)\begin{vmatrix}
      0 & -ab & bc + ac \\
      1 & b + a & -c \\
      0 & c - b & -b + c
     \end{vmatrix} \\
    && &= (b - a)(c - a)(c - b)\begin{vmatrix}
      0 & -ab & bc + ac \\
      1 & b + a & -c \\
      0 & 1 & 1
     \end{vmatrix} \\
    && &= (a - b)(b - c)(c - a)\begin{vmatrix}
      1 & 1 \\
      -ab & bc + ac
     \end{vmatrix} \\
    && &= (a - b)(b - c)(c - a)(ab + bc + ac)
   \end{alignat*}
  \item
   \begin{lemma}[Rule of Sarrus]
    \label{lemma_sarrus}
    The determinant of a \(3 \times 3\) matrix \(M\) is given by
    \begin{align*}
     \begin{vmatrix}
      \color{red}m_{11} & \color{violet}m_{12} & \color{blue}m_{13} \\
      \color{blue}m_{21} & \color{red}m_{22} & \color{violet}m_{23} \\
      \color{violet}m_{31} & \color{blue}m_{32} & \color{red}m_{33}
     \end{vmatrix} =
     \begin{vmatrix}
      \color{teal}m_{11} & \color{olive}m_{12} & \color{orange}m_{13} \\
      \color{olive}m_{21} & \color{orange}m_{22} & \color{teal}m_{23} \\
      \color{orange}m_{31} & \color{teal}m_{32} & \color{olive}m_{33}
     \end{vmatrix} &=
     \begin{aligned}[t]
      &  \textcolor{red}{m_{11} m_{22} m_{33}}
       + \textcolor{violet}{m_{12} m_{23} m_{31}}
       + \textcolor{blue}{m_{13} m_{21} m_{32}} \\
      &-(\textcolor{teal}{m_{11} m_{23} m_{32}}
       + \textcolor{olive}{m_{12} m_{21} m_{33}}
       + \textcolor{orange}{m_{13} m_{22} m_{31}})
     \end{aligned}
    \end{align*}
    This can be read as ``the sum of the leading diagonals minus the sum of the
    trailing diagonals'', considering diagonals wrapping around as if the matrix
    was on the surface of a torus.
   \end{lemma}
   \begin{proof}
    The 6 permutations in \(S_3\) are, in cycle notation:
    \begin{center}
     \begin{tabular}{*4c}
      \bfseries Even & \(\Id\) & \((1\ 3\ 2)\) & \((1\ 2\ 3)\) \\
      \bfseries Odd & \((2\ 3)\) & \((1\ 2)\) & \((1\ 3)\)
     \end{tabular}
    \end{center}
    In the Leibniz definition of the determinant, the even permutations
    then give the terms
    \({m_{11} m_{22} m_{33}}\),
    \({m_{12} m_{23} m_{31}}\),
    \({m_{13} m_{21} m_{32}}\).
    The odd permutations give the terms
    \(m_{11} m_{23} m_{32}\),
    \(m_{12} m_{21} m_{33}\),
    \(m_{13} m_{22} m_{31}\). So this is indeed the correct formula.
   \end{proof}
   So
   \begin{align*}
    \Delta(x, y, z) =
    \begin{vmatrix}
     x & y & z \\
     z & x & y \\
     y & z & x
    \end{vmatrix}
     &= x^3 + y^3 + z^3 - (xyz + yzx + zxy) \\
     &= x^3 + y^3 + z^3 - 3xyz
   \end{align*}
   But also,
   \begin{alignat*}2
    && \Delta(x, y, z) &= \begin{vmatrix}
     x & y & z \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
    \parens*{
     \vec r(1) \to \vec r(1) + \vec r(2) + \vec r(3)
    } \quad
    && &= \begin{vmatrix}
     x + y + z & x + y + z & x + y + z \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
    && &= (x + y + z)\begin{vmatrix}
     1 & 1 & 1 \\
     z & x & y \\
     y & z & x
    \end{vmatrix} \\
   \end{alignat*}
   %TODO
  \item
   Using that for any scalar \(\lambda\) and any \(n \times n\) matrix \(M\),
   \(\det(M) = \det(\tran M)\) and \(\det(\lambda M) = \lambda^n \det(M)\), we
   can show that
   \(\det(A) = \det(\tran A) = \det(-A) = (-1)^{2n + 1}\det(A) = -\det(A)\), so
   \(\det(A)\) must be \(0\).
  \item
   Let \(D_i\) denote the \(i \times i\) matrix of this form, so \(D = D_n\),
   and claim that
   \(\det D_n = (p + n - 1)(p - 1)^{n - 1}\).

   The base case \(n = 1\) is true: \((p + 1 - 1)(p - 1)^0 = p = \det D_1\).

   Also,
   \begin{alignat*}2
    && \det D_n
    &= \begin{vmatrix}
     p & 1 & 1 & 1 & \cdots \\
     1 & p & 1 & 1 & \cdots \\
     1 & 1 & p & 1 & \cdots \\
     \vdots & \vdots & \vdots & \vdots & \ddots
    \end{vmatrix} \\
    \parens*{
     \vec r(i) \to \vec r(i) - \vec r(1), i \ne 1
    }
    && &= \begin{vmatrix}
     p & 1 & 1 & 1 & \cdots \\
     1 - p & p - 1 & 0 & 0 & \cdots \\
     1 - p & 0 & p - 1 & 0 & \cdots \\
     1 - p & 0 & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \vdots & \ddots
    \end{vmatrix} \\
    % this is a bit dirty
    \parens*{
     \begin{gathered}
      \text{Expanding along} \\
      \text{the second row}
     \end{gathered}} \quad
    && &= -(1 - p)\underbrace{\begin{vmatrix}
     1 & 1 & 1 & \cdots \\
     0 & p - 1 & 0 & \cdots \\
     0 & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \ddots \\
    \end{vmatrix}}_{(n - 1) \times (n - 1)} \\
    && &\phantom{={}} + (p - 1)\underbrace{\begin{vmatrix}
     p & 1 & 1 & \cdots \\
     1 - p & p - 1 & 0 & \cdots \\
     1 - p & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \ddots
    \end{vmatrix}}_{(n - 1) \times (n - 1)} \\
    \parens*{
     \begin{gathered}
      \text{Expanding along} \\
      \text{the first row}
     \end{gathered}} \quad
    && &= -(1 - p)\underbrace{\begin{vmatrix}
     p - 1 & 0 & 0 & \cdots \\
     0 & p - 1 & 0 & \cdots \\
     0 & 0 & p - 1 & \cdots \\
     \vdots & \vdots & \vdots & \ddots \\
    \end{vmatrix}}_{(n - 2) \times (n - 2)} \\
    \parens*{
     \vec r(i) \to \vec r(i) + \vec r(1), i \ne 1
    }
    && &\phantom{={}} + (p - 1)\underbrace{\begin{vmatrix}
     p & 1 & 1 & \cdots \\
     1 & p & 1 & \cdots \\
     1 & 1 & p & \cdots \\
     \vdots & \vdots & \vdots & \ddots
    \end{vmatrix}}_{(n - 1) \times (n - 1)} \\
    (\text{by induction hypothesis})\quad
    && &= (p - 1)^{n - 1} + (p - 1) (p + n - 2)(p - 1)^{n - 2} \\
    && &= (p - 1)^{n - 1} + (p + n - 2)(p - 1)^{n - 1} \\
    && &= (p + n - 1)(p - 1)^{n - 1}
   \end{alignat*}
   So we are done, by induction.
  \item
   We present the cofactors as a cofactor matrix \(\Delta\), with entries
   \(\Delta_{ij}\).
   \begin{align*}
    \Delta
    &= \begin{pmatrix*}[r]
     \begin{vmatrix*}[r]
      2 & 3 \\
      -2 & 2
     \end{vmatrix*} &
     -\begin{vmatrix}
      1 & 3 \\
      3 & 2
     \end{vmatrix} &
     \begin{vmatrix*}[r]
      1 & 2 \\
      3 & -2
     \end{vmatrix*} \\[3ex]
     -\begin{vmatrix}
      1 & 1 \\
      -2 & 2
     \end{vmatrix} &
     \begin{vmatrix}
      1 & 1 \\
      3 & 2
     \end{vmatrix} &
     -\begin{vmatrix*}[r]
      1 & 1 \\
      3 & -2
     \end{vmatrix*} \\[3ex]
     \begin{vmatrix}
      1 & 1 \\
      2 & 3
     \end{vmatrix} &
     -\begin{vmatrix}
      1 & 1 \\
      1 & 3
     \end{vmatrix} &
     \begin{vmatrix}
      1 & 1 \\
      1 & 2
     \end{vmatrix}
    \end{pmatrix*} \\
    &= \begin{pmatrix*}[r]
     10 & 7 & -8 \\
     -4 & -1 & 5 \\
     1 & -2 & 1
    \end{pmatrix*}
   \end{align*}
   Now the condition \(A_{ij} \Delta_{ik} = \delta_{jk} \det A\) is equivalent
   to \((\tran \Delta)_{ki} A_{ij} = \delta_{jk} \det A\), ie
   \(\tran \Delta A = (\det A)I\). So let's calculate:
   \begin{equation*}
    \tran \Delta A =
    \begin{pmatrix*}[r]
     10 & -4 & 1 \\
     7 & -1 & -2 \\
     -8 & 5 & 1
    \end{pmatrix*}
    \begin{pmatrix*}[r]
     1 & 1 & 1 \\
     1 & 2 & 3 \\
     3 & -2 & 2
    \end{pmatrix*}
    = \begin{pmatrix}
     9 & 0 & 0 \\
     0 & 9 & 0 \\
     0 & 0 & 9
    \end{pmatrix}
    = 9\mat I
   \end{equation*}
   Using Lemma \ref{lemma_sarrus}, we can also calculate that indeed,
   \(\det \mat A = 4 + 9 - 2 - (6 - 6 + 2) = 9\).

   So
   \begin{equation*}
    \mat A^{-1}
     = \frac 1{\det \mat A} \tran{\mat \Delta}
     = \frac 19
     \begin{pmatrix*}[r]
      10 & -4 & 1 \\
      7 & -1 & -2 \\
      -8 & 5 & 1
     \end{pmatrix*}
   \end{equation*}
   Then
   \begin{alignat*}2
    && x + y + z &= 1 \\
    && x + 2y + 3z &= -5 \\
    && 3x - 2y + 2z &= 4 \\
    \iff{}&&
     \mat A\begin{pmatrix} x \\ y \\ z \end{pmatrix}
      &= \begin{pmatrix*}[r] 1 \\ -5 \\ 4 \end{pmatrix*} \\
    \iff{}&&
     \begin{pmatrix} x \\ y \\ z \end{pmatrix}
      &= \mat A^{-1}\begin{pmatrix*}[r] 1 \\ -5 \\ 4 \end{pmatrix*} \\
    && &=
    \frac 19
    \begin{pmatrix*}[r]
     10 & -4 & 1 \\
     7 & -1 & -2 \\
     -8 & 5 & 1
    \end{pmatrix*}
    \begin{pmatrix*}[r] 1 \\ -5 \\ 4 \end{pmatrix*} \\
    && &=
    \frac 19 \begin{pmatrix*}[r] 34 \\ 4 \\ -29 \end{pmatrix*}
   \end{alignat*}
   and we can check that
   \begin{align*}
    x + y + z &= \tfrac 19(34 + 4 - 29) = \tfrac 19(9) = 1 \\
    x + 2y + 3z &= \tfrac 19(34 + 8 - 87) = \tfrac 19(-45) = -5 \\
    3x - 2y + 2z &= \tfrac 19(102 - 8 - 58) = \tfrac 19(36) = 4
   \end{align*}
  \item \(
   \begin{alignedat}[t]2
    && x + y + z &= t \\
    && tx + 2z &= 3 \\
    && 3x + ty + 5z &= 7 \\
    \iff{}&&
    \begin{pmatrix}
     1 & 1 & 1 \\
     t & 0 & 2 \\
     3 & t & 5
    \end{pmatrix}
    \begin{pmatrix} x \\ y \\ z \end{pmatrix}
     &= \begin{pmatrix} t \\ 3 \\ 7 \end{pmatrix}
   \end{alignedat}\)

   but, using Lemma \ref{lemma_sarrus},
   \begin{equation*}
    \begin{vmatrix}
     1 & 1 & 1 \\
     t & 0 & 2 \\
     3 & t & 5
    \end{vmatrix}
    = 6 + t^2 - 5t - 2t
    = t^2 - 7t + 6
    = (t - 6)(t - 1)
   \end{equation*}
   so this matrix is invertible (ie there is a unique solution) if
   \(t \notin \set{1, 6}\), in which case the solution can be found by some
   elimination, safe in the knowledge that we can divide through by \((t - 1)\)
   and \((t - 6)\):
   \begin{alignat*}2
    && z &= \tfrac 12(3 - tx) \\
    \implies{}&& (1 - \tfrac 12 t)x + y &= t - \tfrac 32 \\
    && (3 - \tfrac 52 t) x + ty &= 7 - \tfrac{15}2 = -\tfrac 12 \\
    \implies{}&& y &= t - \tfrac 32 - (1 - \tfrac 12 t)x \\
    \implies{}&& (3 - \tfrac 52 t) x + t^2 - \tfrac 32 t - (t - \tfrac 12 t^2) x
     &= -\tfrac 12 \\
    \implies{}&& \tfrac 12(6 - 7t + t^2) x &= \tfrac 12(-2t^2 + 3t - 1) \\
    \implies{}&& (t - 6)(t - 1) x &= (1 - 2t)(t - 1) \\
    \implies{}&& x &= \frac{1 - 2t}{t - 6} \\
    \implies{}&& z &= \tfrac 12(3 - tx) \\
    && &= \frac{3t - 18 - t + 2t^2}{2(t - 6)} \\
    && &= \frac{t^2 + t - 9}{t - 6} \\
    \implies{}&& y &= t - x - z \\
    && &= \frac{t^2 - 6t - 1 + 2t - t^2 - t + 9}{t - 6} \\
    && &= \frac{-5t + 8}{t - 6}
   \end{alignat*}
   If, however, \(t = 1\) or \(t = 6\) then there is more work to be done.
   \begin{itemize}
     %TODO spell out operations
    \item If \(t = 1\):
     \begin{alignat*}2
      &&\begin{pmatrix}
       1 & 1 & 1 \\
       1 & 0 & 2 \\
       3 & 1 & 5
      \end{pmatrix}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 3 \\ 7 \end{pmatrix} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -1 & 1 \\
       0 & -2 & 2
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 2 \\ 4 \end{pmatrix} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -1 & 1 \\
       0 & 0 & 0
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}
     \end{alignat*}
     which is consistent, and is solved by letting \(z = \lambda\) be arbitrary,
     letting \(y = \lambda - 2\), and \(x = 1 - y - z = 3 - 2\lambda \). This is
     the line \(\vec x = (3, -2, 0) + \lambda(-2, 1, 0)\)
    \item If \(t = 6\):
     \begin{alignat*}2
      &&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       6 & 0 & 2 \\
       3 & 6 & 5
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ 3 \\ 7 \end{pmatrix} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -6 & -4 \\
       0 & 3 & 2
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix*}[r] 1 \\ -3 \\ 4 \end{pmatrix*} \\
      \iff{}&&\begin{pmatrix*}[r]
       1 & 1 & 1 \\
       0 & -6 & -4 \\
       0 & 0 & 0
      \end{pmatrix*}
      \begin{pmatrix} x \\ y \\ z \end{pmatrix}
       &= \begin{pmatrix} 1 \\ -3 \\ 4 - \tfrac 32 \end{pmatrix}
     \end{alignat*}
     which is inconsistent, as \(4 \ne \tfrac 32\). So there are no solutions.
   \end{itemize}
  \item
   There exist solutions to \(\mat M \vec x = \vec d\) if and only if
   \(\vec d \in \Img \mat M\) (as this is simply the condition that there exist
   vectors which map to \(\vec d\)) , and then they are precisely the vectors of
   the form \(\vec x = \vec x_0 + \vec v\), where \(\vec v \in \Ker \mat M\),
   and \(\vec x_0\) is some particular solution vector such that
   \(\mat M \vec x_0 = \vec d\)
   (which we know must exist if \(\vec d \in \Img \mat M\)).

   These are all solutions as then
   \(\mat M(\vec x_0 + \vec v)
     = \mat M \vec x_0 + \mat M \vec v
     = \vec d + \vec 0 = \vec d\), and furthermore if some
   \(\vec x\) is a solution, then
   \(\mat M(\vec x - \vec x_0)
     = \mat M \vec x - \mat M \vec x_0
     = \vec d - \vec d
     = \vec 0\)
   so indeed \(\vec x - \vec x_0 \in \Ker \mat M\).

   Let's first calculate the determinant of \(\mat M\) in this case:
   \begin{alignat*}2
    &&\det \mat M &=
    \begin{vmatrix}
     1 & 1 & 1 \\
     1 & a & b \\
     1 & a^2 & b^2
    \end{vmatrix} \\
    && &= \begin{vmatrix}
     1 & 0 & 1 \\
     1 & a - b & b \\
     1 & a^2 - b^2 & b^2
    \end{vmatrix} \\
    && &= (a - b)\begin{vmatrix}
     1 & 0 & 1 \\
     1 & 1 & b \\
     1 & a + b & b^2
    \end{vmatrix} \\
    && &= (a - b)\begin{vmatrix}
     1 & 0 & 0 \\
     1 & 1 & b - 1 \\
     1 & a + b & b^2 - 1
    \end{vmatrix} \\
    && &= (a - b)(b - 1)\begin{vmatrix}
     1 & 0 & 0 \\
     1 & 1 & 1 \\
     1 & a + b & b + 1
    \end{vmatrix} \\
    && &= (a - b)(b - 1)\begin{vmatrix}
     1 & 1 \\
     a + b & b + 1
    \end{vmatrix} \\
    && &= (a - b)(b - 1)(1 - a)
   \end{alignat*}
   So:
   \begin{itemize}
    \item
     If \(a \ne b\) and \(a, b \ne 1\), then the image of \(\mat M\) is
     \(\Reals^3\), as the the determinant is nonzero so the columns are linearly
     independent. By the rank-nullity theorem, the kernel of \(\mat M\) is then
     the trivial kernel \(\set{\vec 0}\).
    \item
     If \(a = b\),
     \begin{itemize}
      \item
       If \(a = b = 1\), then \(\Img \mat M\) is simply the span of
       \((1, 1, 1)\), ie the line \(\vec x = \lambda(1, 1, 1)\).

       The kernel of \(\mat M\) is the plane \(\vec x \vecdot (1, 1, 1) = 0\).
      \item
       If \(a, b \ne 1\), then \(\Img \mat M\) is the span of
       \((1, 1, 1)\) and \((1, a, a^2)\), which is the plane
       \begin{alignat*}2
        && \vec x \vecdot ((1, 1, 1) \veccross (1, a, a^2)) &= 0 \\
        \iff{}&& \vec x \vecdot (a^2 - a, 1 - a^2, a - 1) &= 0 \\
        \iff{}&& \vec x \vecdot (a, -(a + 1), 1) &= 0
       \end{alignat*}
       The kernel of \(\mat M\) must be the intersection of the planes
       \(\vec x \vecdot (1, 1, 1) = 0\) and \(\vec x \vecdot(1, a, a) = 0\),
       which is the line
       \begin{alignat*}2
        && \vec x &= \lambda ((1, 1, 1) \veccross (1, a, a)) \\
        \iff{}&& \vec x &= \lambda (0, 1 - a, a - 1) \\
        \iff{}&& \vec x &= \lambda' (0, 1, -1)
       \end{alignat*}
     \end{itemize}
    \item
     If \(a \ne b\) but \(b = 1\), the image is the span of
     \((1, 1, 1)\) and \((1, a, a^2)\), which are linearly independent, so this
     is the plane \(\vec x \vecdot (a, -(a + 1), 1) = 0\), as in the above case
     where \(a = b\).

     The kernel is now the intersection of the planes
     \(\vec x \vecdot (1, 1, 1) = 0\) and \(\vec x \vecdot (1, a, 1) = 0\),
     which is the line
     \begin{alignat*}2
      &&\vec x &= \lambda((1, 1, 1) \veccross (1, a, 1)) \\
      \iff{}&&\vec x &= \lambda(1 - a, 0, a - 1) \\
      \iff{}&&\vec x &= \lambda'(1, 0, -1)
     \end{alignat*}
    \item
      If \(a \ne b\) but \(a = 1\), the image is turns out exactly the same as
      in the previous case: the plane
      \(\vec x \vecdot (b, -(b + 1), 1) = 0\).

      The kernel is now the intersection of the planes
      \(\vec x \vecdot(1, 1, 1) = 0\) and \(\vec x \vecdot (1, 1, b) = 0\),
      which is the line
     \begin{alignat*}2
      &&\vec x &= \lambda((1, 1, 1) \veccross (1, 1, b)) \\
      \iff{}&&\vec x &= \lambda(b - 1, 1, - b, 0) \\
      \iff{}&&\vec x &= \lambda'(1, -1, 0)
     \end{alignat*}
   \end{itemize}
   \begin{enumerate}[label=(\roman*)]
    \item
     As noted earlier, \(\mat M\) is non-singular precisely if
     \(a \ne b\) and \(a, b \ne 1\), in which case there is a unique solution.
    \item \label{item_ii}
     For there to be more than one solution, we need \(\mat M\) to be singular
     (so that its kernel is nontrivial) and \(\vec d \in \Img \mat M\).

     Revisiting each case,
     \begin{itemize}
      \item
       If \(a = b\),
       \begin{itemize}
        \item
         If \(a = b = 1\), there is no solution, as
         \(\vec d\) is not of the form \(\lambda(1, 1, 1)\).
        \item
         If \(a, b \ne 1\), then
         \begin{alignat*}2
          &&\vec d &\in \Img \mat M \\
          \iff{}&& \vec d \vecdot (a, -(a + 1), 1) &= 0 \\
          \iff{}&& 2a + 2 &= 0 \\
          \iff{}&& a &= -1
         \end{alignat*}
         so \textbf{there are solutions} if
         \(a = b = -1\). The solutions are then the line
         \(\vec x = (0, 1, 0) + \lambda(0, 1, -1)\).
       \end{itemize}
      \item
       If \(a \ne b\) but \(b = 1\), then the image remains the same,
       so \textbf{there are solutions} if \(a = -1\) and \(b = 1\), and the
       solutions are then the line
       \(\vec x = (0, 1, 0) + \lambda(1, 0, -1)\)
      \item
       Similarly, \textbf{there are solutions} if
       \(a = 1\) and \(b = -1\), given by the line
       \(\vec x = (0, 0, 1) + \lambda(1, -1, 0)\).
     \end{itemize}
    \item
     This is the complement to part \ref{item_ii}. There are no solutions if
     \(a\) and \(b\) are not both either \(1\) or \(-1\), or both are \(1\).
   \end{enumerate}
  \item
   \begin{enumerate}[label=(\alph*)]
    \item
     We claim that the rotation matrix
     \begin{equation*}
      \mat R =
      \begin{pmatrix*}[r]
       0 & -1 & 0 \\
       1 & 0 & 0 \\
       0 & 0 & 1 \\
      \end{pmatrix*}
     \end{equation*}
     has eigenvalues \(1, \pm i\). If you happen to have been told in lectures
     that the eigenvalues of a \(2 \times 2\) rotation matrix by an angle
     \(\theta\) are \(e^{\pm i\theta}\), then this is fairly obvious. If in
     doubt, we can always use Lemma \ref{lemma_sarrus} to find and solve the
     characteristic polynomial:
     \begin{align*}
      \chi_{\mat R}(t)
       &= \det(\mat R - t\mat I) \\
       &= \begin{vmatrix}
        -t & -1 & 0 \\
        1 & -t & 0 \\
        0 & 0 & 1 - t
       \end{vmatrix} \\
       &= t^2(1 - t) + (1 - t) \\
       &= (t^2 + 1)(1 - t) \\
       &= (t + i)(t - i)(1 - t)
     \end{align*}
     and we are done.
    \item
     We claim that the matrix
     \begin{equation*}
      \mat B =
      \begin{pmatrix}
       0 & 1 & 0 \\
       0 & 0 & 0 \\
       0 & 0 & 0
      \end{pmatrix}
     \end{equation*}
     has all three eigenvalues zero. Let's use Lemma \ref{lemma_sarrus} to find
     its characteristic polynomial:
     \begin{align*}
      \chi_{\mat B}(t)
       &= \det(\mat B - t \mat I) \\
       &= \begin{vmatrix}
        -t & 1 & 0 \\
        0 & -t & 0 \\
        0 & 0 & -t
       \end{vmatrix} \\
       &= -t^3
     \end{align*}
     It has been cleverly constructed so that every diagonal other than the
     leading one of \(\mat B - t \mat I\) has a zero, resulting in a
     characteristic polynomial of \(-t^3\), which has all three roots \(0\).
    \item
     Let \(\lambda\) be an arbitrary eigenvalue of \(\mat A\). Then, by
     definition, for the corresponding eigenvector \(\vec v \ne \vec 0\), we
     have \(\mat A \vec v = \lambda \vec v\). But then \footnote{by induction}
     \(\mat A^m \vec v = \lambda^m \vec v\), while also
     \(\mat A^m \vec v = \mat 0 \vec v = \vec 0\). But
     \(\vec v \ne \vec 0\), so \(\lambda^m = 0\), so \(\lambda = 0\).

     So all the eigenvalues of \(\mat A\) are \(0\).
    \item
     Let
     \begin{equation*}
      \mat M = \begin{pmatrix}
       m_{11} & m_{12} \\
       m_{21} & m_{22}
      \end{pmatrix},
     \end{equation*}
     so
     \begin{align*}
      \chi_{\mat M}(t) &= \det(\mat M - t \mat I) \\
       &= \begin{vmatrix}
       m_{11} - t & m_{12} \\
       m_{21} & m_{22} - t
      \end{vmatrix} \\
       &= (m_{11} - t)(m_{22} - t) - m_{12} m_{21}
     \end{align*}
   \end{enumerate}
 \end{enumerate}
\end{document}
